{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAPE Calcium Imaging Event-Related Analysis\n",
    "\n",
    "## What does this script do:\n",
    "\n",
    "Loads activity traces from ROIs/sources across the whole session as well as timing of behavioral/manipulation events, then extracts a window of activity around each event, and finally generates numerous event-related plots resolving and averaging across trials and ROIs.\n",
    "\n",
    "__Important__: when setting variables such as `trial_start_end` and `baseline_start_end`, the numbers supplied refer to time _relative_ to the onset of events. For example if you wanted to pull out activity traces in a 2 second window before and 3 second window after an event, `trial_start_end` would be a list like so: [-2, 3]. \n",
    "* Related to this, by default, this code will perform baseline normalization using the `baseline_start_end` variable. By default, the values are also relative to the event onset (similar to `trial_start_end`); however, if you want to perform normalization using a specific time window for ALL events, set `specific_baseline` to True and `baseline_start_end` should now refer to a specific window in seconds relative to the start of the whole recording.\n",
    "\n",
    "How to run this code\n",
    "------------------------------------\n",
    "\n",
    "In this jupyter notebook, First find the code block with the comment header called USER-DEFINED VARIABLES. Edit the variables according to your data and output preferences. Then just run all cells in order (shift + enter; or in the dropdown menu: Kernel->Resart & Run All). Please make sure you set the correct sampling rate (fs) in the user-defined variables block of code.\n",
    "\n",
    "### Required Prerequisite/Input Files\n",
    "\n",
    "All data should reside in a parent folder. This folder's name should be the name of the session and ideally be the same as the base name of the recording file.\n",
    "\n",
    "1. ROI/source activity signals file ( `fparams['fname_signal']` ): This is either a npy or CSV file where rows are individual ROIs/sources and columns are samples, and the values are activity levels (ie. fluorescence or voltage for ephys). Note the CSV should not \n",
    "2. Event occurrence file (`fparams['fname_events']` ): This is either a pickle file or a CSV file. For the pickle file, it should contain a python dictionary where keys are event condition names and associated values are lists that contain event occurrence times (in samples). If using a CSV, the data should be in tidy format formated like what is shown here (note currently the first row, first and second columns should be \"event\" and \"sample\" respectively: https://github.com/zhounapeuw/NAPE_imaging_postprocess/raw/main/docs/_images/napeca_post_event_csv_format.png\n",
    "\n",
    "\n",
    "Required Packages\n",
    "-----------------\n",
    "Python 3.6, seaborn, matplotlib, pandas, scikit-learn\n",
    "\n",
    "Custom code requirements: utils\n",
    "\n",
    "User-Defined Parameters \n",
    "----------\n",
    "\n",
    "fname_signal : string\n",
    "    \n",
    "    Name of file that contains roi activity traces. Must include full file name with extension. Accepted file types: .npy, .csv. IMPORTANT: data dimensions should be rois (y) by samples/time (x)\n",
    "\n",
    "fname_events : string\n",
    "\n",
    "    Name of file that contains event occurrences. Must include full file name with extension. Accepted file types: .pkl, .csv. Pickle (pkl) files need to contain a dictionary where keys are the condition names and the values are lists containing samples/frames for each corresponding event. Csv's should have two columns (event condition, sample). The first row are the column names. Subsequent rows contain each trial's event condition and sample in tidy format. See example in sample_data folder for formatting, or this link: https://github.com/zhounapeuw/NAPE_imaging_postprocess/raw/main/docs/_images/napeca_post_event_csv_format.png\n",
    "\n",
    "fdir : string \n",
    "\n",
    "    Root file directory containing the raw tif, tiff, h5 files. IMPORTANT Note: leave off the last backslash, and include the letter r in front of string (treats the contents as a raw string). For example: r'C:\\Users\\my_user\\analyze_sessions'\n",
    "\n",
    "fname : string\n",
    "\n",
    "    Session name; by default this is the name of the parent folder that the data resides in, but can be changed by user to be any string. This fname variable is mainly used to name the saved output files.\n",
    "\n",
    "flag_save_figs : boolean  \n",
    "\n",
    "    Set as True to save figures as JPG and vectorized formats.  \n",
    "    \n",
    "fs : float\n",
    "\n",
    "    Sampling rate of imaging data. It is imperative that this value is correct; otherwise the incorrect time windows will be pulled out for each event. If you suspect that \n",
    "    the sampling rate (fs) was not set correctly in the NAPECA preprocessing pipeline, go into the saved json file and edit the fs value.\n",
    "    \n",
    "selected_conditions : list of strings\n",
    "\n",
    "    Specific conditions that the user wants to analyze; needs to be exactly the name of conditions in the events CSV or pickle file\n",
    "\n",
    "trial_start_end : list of two entries  \n",
    "\n",
    "    Entries can be ints or floats. The first entry is the time in seconds relative to the event/ttl onset for the start of the event analysis window (negative if start time is before the event/ttl onset). Event analysis window refers to the primary time window around events that is visualized in plots. The second entry is the time in seconds for the end of the event analysis window. For example if the desired analysis window is 5.5 seconds before event onset and 8 seconds after, `trial_start_end` would be [-5.5, 8].  \n",
    "\n",
    "specific_baseline : boolean\n",
    "\n",
    "    Set to False for the default setting of using `baseline_start_end` as described below. If set to True, the first and second entry of `baseline_start_end` will now reference a start/end of the time window for the data relative to the onset of the WHOLE recording, AND all trials will be normalized to this specific time window. This is useful if your baseline period is a single window at, for example, the beginning of your recording.\n",
    "\n",
    "baseline_start_end : list of two entries  \n",
    "\n",
    "    Entries can be ints or floats. The first entry is the time in seconds relative to the event/ttl onset for the start of the baseline window (negative if start time is before the event/ttl onset). Baseline window refers to the time window relative to event onset that is used to calculate the mean and/or standard deviation for normalization. The second entry is the time in seconds (relative to event onset) for the end of the baseline window. For example if the desired analysis window is 5.5 seconds to 0.2 seconds before event onset, `baseline_start_end` would be [-5.5, -0.2]. If a single number is supplied, the baseline window onset will default to the first entry of `trial_start_end` and window end will be the value supplied to `baseline_start_end`.\n",
    "    \n",
    "event_dur : int/float  \n",
    "\n",
    "    Time in seconds representing how long the behavioral event or stimulation, etc. lasts. A green line with the corresponding length will be plotted indicating stimulus duration. This is purely for visualization purposes.\n",
    "\n",
    "event_sort_analysis_win : list with two float entries\n",
    "\n",
    "    Time window [a, b] in seconds during which some visualization calculations will apply to. This can be the whole window or a subset. For example, if the user sets flag_sort_rois to be True, ROIs in heatmaps will be sorted based on the mean activity in the time window between a and b. Similar principle holds for the time-averaged barplots.\n",
    "\n",
    "opto_blank_frame : boolean\n",
    "\n",
    "    if PMTs were blanked during stim, use detected stim times (from preprocessing) to set those frames to NaN\n",
    "\n",
    "flag_npil_corr : boolean  \n",
    "\n",
    "    Set as True if user would like to load in neuropil corrected data from the preprocessing pipeline. Must have a \\*\\_neuropil\\_corrected_signal_* file in the directory. If set as False, just use the extracted_signal file.\n",
    "    \n",
    "flag_zscore : boolean  \n",
    "\n",
    "    Set as True if analyzed data should be baseline z-scored on a trial level.  \n",
    "\n",
    "flag_sort_rois : boolean\n",
    "\n",
    "    Set as True to sort ROIs on the y axis of heatmaps. This works with `user_sort_method` and `roi_sort_cond` for specifying details of sorting.\n",
    "\n",
    "user_sort_method : string\n",
    "    \n",
    "    Set to 'peak_time' to sort ROIs by peak time during event_sort_dur; 'max_value' to sort by max value\n",
    "    \n",
    "roi_sort_cond : string\n",
    "    \n",
    "    Condition to perform sorting on\n",
    "\n",
    "flag_roi_trial_avg_errbar : boolean  \n",
    "\n",
    "    Set as True to set standard error of mean shaded portions for line plots of trial-averaged activity.   \n",
    "\n",
    "flag_trial_avg_errbar : boolean\n",
    "\n",
    "    toggle standard error bars on trial-averaged data\n",
    "\n",
    "interesting_rois : list of ints  \n",
    "\n",
    "    All entries are indices for ROIs that will be marked in heatmaps by arrows.  \n",
    "\n",
    "Optional Parameters (Only relevant if using batch_process)\n",
    "----------------------------------------------------------\n",
    "\n",
    "user_sort_method : string\n",
    "    \n",
    "    Takes the strings 'peak_time' or 'max_value'\n",
    "    \n",
    "    \n",
    "roi_sort_cond : string\n",
    "    for roi-resolved heatmaps, which condition to sort ROIs by\n",
    "    \n",
    "    Defaults to first condition available\n",
    "    \n",
    "Output\n",
    "------\n",
    "\n",
    "event_rel_analysis : folder containing plots in jpg/png and vectorized formats\n",
    "\n",
    "event_data_dict.pkl : pickle file containing a dictionary containing event-triggered data from each cell and organized by event condition. Raw and z-scored data are included as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#important for text to be detected when importing saved figures into illustrator\n",
    "matplotlib.rcParams['pdf.fonttype']=42\n",
    "matplotlib.rcParams['ps.fonttype']=42\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple class to update limits as you go through iterations of data\n",
    "# first call update_lims(first_lims)\n",
    "# then update_lims.update(new_lims)\n",
    "# update_lims.output() outputs lims\n",
    "class update_lims:\n",
    "    \n",
    "    def __init__(self, lims):\n",
    "        self.lims = lims\n",
    "        \n",
    "    \n",
    "    def update(self, new_lims):\n",
    "        if self.lims[0] > new_lims[0]:\n",
    "            self.lims[0] = new_lims[0]\n",
    "        \n",
    "        if self.lims[1] < new_lims[1]:\n",
    "            self.lims[1] = new_lims[1]\n",
    "\n",
    "    def output(self):\n",
    "        return self.lims\n",
    "    \n",
    "    \n",
    "# find 2D subplot index based on a numerical incremented index (ie. idx=3 would be (2,1) for a 2x2 subplot figure)     \n",
    "def subplot_loc(idx, num_rows, num_col):\n",
    "    if n_rows == 1:\n",
    "        subplot_index = idx\n",
    "    else:\n",
    "        subplot_index = np.unravel_index(idx, (n_rows, int(n_columns))) # turn int index to a tuple of array coordinates\n",
    "    return subplot_index\n",
    "\n",
    "\n",
    "def get_cmap(n, name='plasma'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "\n",
    "\n",
    "def is_all_nans(vector):\n",
    "    \"\"\"\n",
    "    checks if series or vector contains all nans; returns boolean. Used to identify and exclude all-nan rois\n",
    "    \"\"\"\n",
    "    if isinstance(vector, pd.Series):\n",
    "        vector = vector.values\n",
    "    return np.isnan(vector).all()\n",
    "\n",
    "# declare some fixed constant variables\n",
    "axis_label_size = 15\n",
    "tick_font_size = 14 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "USER-DEFINED VARIABLES\n",
    "\"\"\"\n",
    "\n",
    "def define_params(method = 'single'):\n",
    "    \n",
    "    fparams = {}\n",
    "    \n",
    "    if method == 'single':\n",
    "        \n",
    "        fparams['fname_signal'] = 'VJ_OFCVTA_7_260_D6_neuropil_corrected_signals_15_50_beta_0.8.npy'   # \n",
    "        fparams['fname_events'] = 'event_times_VJ_OFCVTA_7_260_D6_trained.csv'\n",
    "        # fdir signifies to the root path of the data. Currently, the abspath phrase points to sample data from the repo.\n",
    "        # To specify a path that is on your local computer, use this string format: r'your_root_path', where you should copy/paste\n",
    "        # your path between the single quotes (important to keep the r to render as a complete raw string). See example below:\n",
    "        # r'C:\\Users\\stuberadmin\\Documents\\GitHub\\NAPE_imaging_postprocess\\napeca_post\\sample_data' \n",
    "        fparams['fdir'] = os.path.abspath('./sample_data/VJ_OFCVTA_7_260_D6') \n",
    "        fparams['fname'] = os.path.split(fparams['fdir'])[1]\n",
    "        fparams['flag_close_figs_after_save'] = True\n",
    "        fparams['flag_save_figs'] = True\n",
    "        \n",
    "        # set the sampling rate\n",
    "        fparams['fs'] = 5 # this gets overwritten by json fs variable (if it exists) that is saved in preprocessing\n",
    "        json_fpath = os.path.join(fparams['fdir'], fparams['fname']+\".json\")\n",
    "        if os.path.exists(json_fpath):\n",
    "            json_data = utils.open_json(json_fpath)\n",
    "            if 'fs' in json_data:\n",
    "                fparams['fs'] = json_data['fs']\n",
    "\n",
    "        # set to None if want to include all conditions from behav data; \n",
    "        # otherwise, set to list of conditions, eg. ['plus', 'minus']\n",
    "        fparams['selected_conditions'] = None \n",
    "        \n",
    "        # trial windowing and normalization\n",
    "        fparams['trial_start_end'] = [-2, 8] # primary visualization window relative to event onset; [start, end] times (in seconds) \n",
    "        fparams['flag_normalization'] = 'zscore' # options: 'zscore', None\n",
    "        fparams['specific_baseline'] = False\n",
    "        fparams['baseline_start_end'] = [-2, -0.2] # baseline window (in seconds) for performing baseline normalization. either a list [start, end] or an int/float (see details in markdown above); I set this to -0.2 to be safe I'm not grabbing a sample that includes the event\n",
    "        fparams['event_dur'] = 2 # duration of stim/event in seconds; displays a line below main plot indicating event duration\n",
    "        fparams['event_sort_analysis_win'] = [0, 5] # time window (in seconds) for sorting cells; list [start, end]\n",
    "        \n",
    "        # session info\n",
    "        fparams['opto_blank_frame'] = False # if PMTs were blanked during stim, set stim times to nan (instead of 0)\n",
    "\n",
    "        # ROI sorting; if flag_sort_rois is set to True, ROIs are sorted by activity in the fparams['event_sort_analysis_win'] window\n",
    "        fparams['flag_sort_rois'] = True\n",
    "        if fparams['flag_sort_rois']:\n",
    "            \n",
    "            fparams['user_sort_method'] = 'max_value' # peak_time or max_value\n",
    "            fparams['roi_sort_cond'] = 'plus' # for roi-resolved heatmaps, which condition to sort ROIs by\n",
    "            \n",
    "        # errorbar and saving figures\n",
    "        fparams['flag_roi_trial_avg_errbar'] = True # toggle to show error bar on roi- and trial-averaged traces\n",
    "        fparams['flag_trial_avg_errbar'] = True # toggle to show error bars on the trial-avg traces\n",
    "        fparams['interesting_rois'] = [] #[ 0, 1, 2, 23, 22, 11, 9, 5, 6, 7, 3, 4, 8, 12, 14, 15, 16, 17] # [35, 30, 20, 4] #\n",
    "    \n",
    "    elif method == 'f2a': # if string is empty, load predefined list of files in files_to_analyze_event\n",
    "\n",
    "        fparams = files_to_analyze_event.define_fparams()\n",
    "\n",
    "    elif method == 'root_dir':\n",
    "        \n",
    "        pass\n",
    "\n",
    "    with open(os.path.join(fparams['fdir'], 'event_analysis_fparam.json'), 'w') as fp:\n",
    "        json.dump(fparams, fp)\n",
    "    \n",
    "    return fparams\n",
    "\n",
    "fparams = define_params(method = 'single') # options are 'single', 'f2a', 'root_dir'\n",
    "\n",
    "if 'zscore' in fparams['flag_normalization']:\n",
    "    data_trial_resolved_key = 'zdata'\n",
    "    data_trial_avg_key = 'ztrial_avg_data'\n",
    "    cmap_ = None\n",
    "    ylabel = 'Z-score Activity'\n",
    "else:\n",
    "    data_trial_resolved_key = 'data'\n",
    "    data_trial_avg_key = 'trial_avg_data'\n",
    "    cmap_ = 'inferno'\n",
    "    ylabel = 'Activity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare paths\n",
    "signals_fpath = os.path.join(fparams['fdir'], fparams['fname_signal'])\n",
    "events_file_path = os.path.join(fparams['fdir'], fparams['fname_events'])\n",
    "\n",
    "save_dir = os.path.join(fparams['fdir'], 'event_rel_analysis')\n",
    "\n",
    "utils.check_exist_dir(save_dir); # make the save directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create variables that reference samples and times for slicing and plotting the data\n",
    "\n",
    "trial_start_end_sec = np.array(fparams['trial_start_end']) # trial windowing in seconds relative to ttl-onset/trial-onset, in seconds\n",
    "if type(fparams['baseline_start_end']) is list:\n",
    "    baseline_start_end_sec = np.array(fparams['baseline_start_end'])\n",
    "elif isinstance(fparams['baseline_start_end'], (int, float)):\n",
    "    baseline_start_end_sec = np.array([trial_start_end_sec[0], fparams['baseline_start_end']])\n",
    "\n",
    "# convert times to samples and get sample vector for the trial \n",
    "trial_begEnd_samp = np.round(trial_start_end_sec*fparams['fs']).astype('int') # turn trial start/end times to samples\n",
    "trial_svec = np.arange(trial_begEnd_samp[0], trial_begEnd_samp[1])\n",
    "# and for baseline period\n",
    "baseline_begEnd_samp = np.round(baseline_start_end_sec*fparams['fs']).astype('int')\n",
    "baseline_svec = (np.arange(baseline_begEnd_samp[0], baseline_begEnd_samp[1]+1, 1) - baseline_begEnd_samp[0])\n",
    "\n",
    "# calculate time vector for plot x axes\n",
    "num_samples_trial = len( trial_svec )\n",
    "tvec = np.round(np.linspace(trial_start_end_sec[0], trial_start_end_sec[1], num_samples_trial+1), 2)\n",
    "\n",
    "# find samples and calculations for time 0 for plotting\n",
    "t0_sample = utils.get_tvec_sample(tvec, 0) # grabs the sample index of a given time from a vector of times\n",
    "event_end_sample = int(np.round(t0_sample+fparams['event_dur']*fparams['fs']))\n",
    "event_bound_ratio = [(t0_sample)/num_samples_trial , event_end_sample/num_samples_trial] # fraction of total samples for event start and end; only used for plotting line indicating event duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = utils.load_signals(signals_fpath)\n",
    "\n",
    "num_rois = signals.shape[0]\n",
    "all_nan_rois = np.where(np.apply_along_axis(is_all_nans, 1, signals)) # find rois with activity as all nans\n",
    "\n",
    "# if opto stim frames were detected in preprocessing, set these frames to be NaN (b/c of stim artifact)\n",
    "if fparams['opto_blank_frame']:\n",
    "    try:\n",
    "        glob_stim_files = glob.glob(os.path.join(fparams['fdir'], \"{}*_stimmed_frames.pkl\".format(fparams['fname'])))\n",
    "        stim_frames = pickle.load( open( glob_stim_files[0], \"rb\" ) )\n",
    "        signals[:,stim_frames['samples']] = None # blank out stimmed frames\n",
    "        flag_stim = True\n",
    "        print('Detected stim data; replaced stim samples with NaNs')\n",
    "    except:\n",
    "        flag_stim = False\n",
    "        print('Note: No stim preprocessed meta data detected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load behavioral data and trial info\n",
    "\n",
    "glob_event_files = glob.glob(events_file_path) # look for a file in specified directory\n",
    "if not glob_event_files:\n",
    "    print(f'{events_file_path} not detected. Please check if path is correct.')\n",
    "if 'csv' in glob_event_files[0]:\n",
    "    event_times = utils.df_to_dict(glob_event_files[0])\n",
    "elif any(x in glob_event_files[0] for x in ['pkl', 'pickle']):\n",
    "    event_times = pickle.load( open( glob_event_files[0], \"rb\" ), fix_imports=True, encoding='latin1' ) # latin1 b/c original pickle made in python 2\n",
    "event_frames = utils.dict_time_to_samples(event_times, fparams['fs'])\n",
    "\n",
    "# identify conditions to analyze\n",
    "all_conditions = event_frames.keys()\n",
    "conditions = [ condition for condition in all_conditions if len(event_frames[condition]) > 0 ] # keep conditions that have events\n",
    "\n",
    "conditions.sort()\n",
    "if fparams['selected_conditions']:\n",
    "    conditions = fparams['selected_conditions']\n",
    "\n",
    "cmap_lines = get_cmap(len(conditions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start trial-based preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MAIN data processing function to extract event-centered data\n",
    "\n",
    "extract and save trial data, \n",
    "saved data are in the event_rel_analysis subfolder, a pickle file that contains the extracted trial data\n",
    "\"\"\"\n",
    "data_dict = utils.extract_trial_data(signals, tvec, trial_begEnd_samp, event_frames, \n",
    "                                     conditions, fparams['specific_baseline'], baseline_start_end_samp = baseline_begEnd_samp, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate all the color limits for heatmaps; useful for locking color limits across different heatmap subplots   \n",
    "def generate_clims(data_in, norm_type):\n",
    "    # get min and max for all data across conditions \n",
    "    clims_out = [np.nanmin(data_in), np.nanmax(data_in)]\n",
    "    if 'zscore' in norm_type: # if data are zscored, make limits symmetrical and centered at 0\n",
    "        clims_max = np.max(np.abs(clims_out)) # then we take the higher of the two magnitudes\n",
    "        clims_out = [-clims_max*0.5, clims_max*0.5] # and set it as the negative and positive limit for plotting\n",
    "    return clims_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot trial-resolved heatmap for each ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_trial_heatmap(data_in, tvec, event_bound_ratio, clims, title, subplot_index, cmap_='inferno', save_fig = False, axis_label_size=15):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    data_in : np.array with dimensions trials x samples\n",
    "        \n",
    "    \n",
    "    Prep information about specific condition (each loop) and plot heatmap\n",
    "        1) x/y label tick values\n",
    "        2) x/y labels\n",
    "        3) grab specific condition's data\n",
    "        4) plot data (using utils function)\n",
    "        5) plot meta data lines (eg. 0-time line, event duration line)\n",
    "\n",
    "    event_bound_ratio : list of two entries\n",
    "        where entries are fraction of total samples for event start and end. Used for mapping which samples to plot green line \n",
    "        indicating event duration\n",
    "    \"\"\"\n",
    "\n",
    "    # set imshow extent to replace x and y axis ticks/labels\n",
    "    plot_extent = [tvec[0], tvec[-1], data_in.shape[0], 0] # [x min, x max, y min, ymax]\n",
    "\n",
    "    # prep labels; plot x and y labels for first subplot\n",
    "    if subplot_index == (0, 0) or subplot_index == 0 :\n",
    "        ax[subplot_index].set_ylabel('Trial', fontsize=axis_label_size)\n",
    "        ax[subplot_index].set_xlabel('Time [s]', fontsize=axis_label_size);\n",
    "    ax[subplot_index].tick_params(axis = 'both', which = 'major', labelsize = tick_font_size)\n",
    "\n",
    "    # prep the data\n",
    "    to_plot = np.squeeze(data_in) \n",
    "    if len(event_frames[cond]) == 1: # accomodates single trial data\n",
    "        to_plot = to_plot[np.newaxis, :]\n",
    "\n",
    "    # plot the data\n",
    "    im = utils.subplot_heatmap(ax[subplot_index], title, to_plot, cmap=cmap_, clims=clims, extent_=plot_extent)\n",
    "\n",
    "    # add meta data lines\n",
    "    ax[subplot_index].axvline(0, color='0.5', alpha=1) # plot vertical line for time zero\n",
    "    # plots green horizontal line indicating event duration\n",
    "    ax[subplot_index].annotate('', xy=(event_bound_ratio[0], -0.01), xycoords='axes fraction', \n",
    "                               xytext=(event_bound_ratio[1], -0.01), \n",
    "                               arrowprops=dict(arrowstyle=\"-\", color='g'))\n",
    "\n",
    "    cbar = fig.colorbar(im, ax = ax[subplot_index], shrink = 0.5)\n",
    "    cbar.ax.set_ylabel(ylabel, fontsize = axis_label_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subplots = len(conditions) + 1 # plus one for trial-avg traces\n",
    "n_columns = np.min([num_subplots, 4.0])\n",
    "n_rows = int(np.ceil(num_subplots/n_columns))\n",
    "\n",
    "for iROI in range(num_rois):\n",
    "     \n",
    "    # calculate color limits. This is outside of heatmap function b/c want lims across conditions\n",
    "    # loop through each condition's data and flatten before concatenating values\n",
    "    roi_clims = generate_clims(np.concatenate([data_dict[cond][data_trial_resolved_key][:, iROI, :].flatten() for cond in conditions]), \n",
    "                               fparams['flag_normalization'])\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=n_rows, ncols=int(n_columns), \n",
    "                           figsize=(n_columns*4, n_rows*3),\n",
    "                           constrained_layout=True)\n",
    "    \n",
    "    ### Plot heatmaps for each condition\n",
    "    for idx_cond, cond in enumerate(conditions):\n",
    "        \n",
    "        subplot_index = subplot_loc(idx_cond, n_rows, n_columns) # determine subplot location index\n",
    "        data_to_plot = data_dict[cond][data_trial_resolved_key][:, iROI, :]\n",
    "        title = 'ROI {}; {}'.format(str(iROI), cond)\n",
    "        \n",
    "        subplot_trial_heatmap(data_to_plot, tvec, event_bound_ratio, roi_clims, title, subplot_index, cmap_, save_fig=False)\n",
    "    \n",
    "    ### plot last subplot of trial-avg traces\n",
    "    \n",
    "    # determine subplot location index\n",
    "    subplot_index = subplot_loc(num_subplots-1, n_rows, n_columns)\n",
    "\n",
    "    for cond in conditions:\n",
    "        \n",
    "        # prep data to plot\n",
    "        num_trials = data_dict[cond]['num_trials']\n",
    "        to_plot = np.nanmean(data_dict[cond][data_trial_resolved_key][:,iROI,:], axis=0)\n",
    "        to_plot_err = np.nanstd(data_dict[cond][data_trial_resolved_key][:,iROI,:], axis=0)/np.sqrt(num_trials)\n",
    "        \n",
    "        # plot trace\n",
    "        ax[subplot_index].plot(tvec, to_plot)\n",
    "        if fparams['opto_blank_frame']: \n",
    "            ax[subplot_index].plot(tvec[t0_sample:event_end_sample], to_plot[t0_sample:event_end_sample], marker='.', color='g')\n",
    "        # plot shaded error\n",
    "        if fparams['flag_trial_avg_errbar']:\n",
    "            ax[subplot_index].fill_between(tvec, to_plot - to_plot_err, to_plot + to_plot_err,\n",
    "                         alpha=0.5) # this plots the shaded error bar\n",
    "        \n",
    "    # plot x, y labels, and legend\n",
    "    ax[subplot_index].set_ylabel(ylabel, fontsize=axis_label_size)\n",
    "    ax[subplot_index].set_xlabel('Time [s]', fontsize=axis_label_size)\n",
    "    ax[subplot_index].set_title('ROI # {}; Trial-avg'.format(str(iROI)), fontsize=axis_label_size)\n",
    "    ax[subplot_index].legend(conditions)\n",
    "    ax[subplot_index].autoscale(enable=True, axis='both', tight=True)\n",
    "    ax[subplot_index].axvline(0, color='0.5', alpha=0.65) # plot vertical line for time zero\n",
    "    ax[subplot_index].annotate('', xy=(event_bound_ratio[0], -0.01), xycoords='axes fraction', \n",
    "                                   xytext=(event_bound_ratio[1], -0.01), \n",
    "                                   arrowprops=dict(arrowstyle=\"-\", color='g'))\n",
    "    ax[subplot_index].tick_params(axis = 'both', which = 'major', labelsize = tick_font_size)\n",
    "    \n",
    "    for a in ax.flat[num_subplots:]:\n",
    "        a.axis('off')\n",
    "    \n",
    "    if fparams['flag_save_figs']:\n",
    "        fig.savefig( os.path.join(save_dir,'roi_{}_activity.png'.format(str(iROI))) ); \n",
    "        fig.savefig( os.path.join(save_dir,'roi_{}_activity.pdf'.format(str(iROI))) );\n",
    "    \n",
    "    if fparams['flag_close_figs_after_save']:\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find closest sample when a time occurs in a time vector\n",
    "tvec2samp = lambda tvec, time: np.argmin(np.abs(tvec - time))\n",
    "\n",
    "# function to sort ROIs based on activity in certain epoch\n",
    "def sort_heatmap_peaks(data, tvec, sort_epoch_start_time, sort_epoch_end_time, sort_method = 'peak_time'):\n",
    "    \n",
    "    # find start/end samples for epoch\n",
    "    sort_epoch_start_samp = tvec2samp(tvec, sort_epoch_start_time)\n",
    "    sort_epoch_end_samp = tvec2samp(tvec, sort_epoch_end_time)\n",
    "    \n",
    "    if sort_method == 'peak_time':\n",
    "        epoch_peak_samp = np.argmax(data[:,sort_epoch_start_samp:sort_epoch_end_samp], axis=1)\n",
    "        final_sorting = np.argsort(epoch_peak_samp)\n",
    "    elif sort_method == 'max_value':\n",
    " \n",
    "        time_max = np.nanmax(data[:,sort_epoch_start_samp:sort_epoch_end_samp], axis=1)\n",
    "        final_sorting = np.argsort(time_max)[::-1]\n",
    "\n",
    "    return final_sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if flag is true, sort ROIs (usually by average fluorescence within analysis window)\n",
    "if fparams['flag_sort_rois']:\n",
    "    if not fparams['roi_sort_cond']: # if no condition to sort by specified, use first condition\n",
    "        fparams['roi_sort_cond'] = data_dict.keys()[0]\n",
    "    if not fparams['roi_sort_cond'] in data_dict.keys():\n",
    "        sorted_roi_order = range(num_rois)\n",
    "        interesting_rois = fparams['interesting_rois']\n",
    "        print('Specified condition to sort by doesn\\'t exist! ROIs are in default sorting.')\n",
    "    else:\n",
    "        # returns new order of rois sorted using the data and method supplied in the specified window\n",
    "        sorted_roi_order = sort_heatmap_peaks(data_dict[fparams['roi_sort_cond']]['ztrial_avg_data'], tvec, \n",
    "                           sort_epoch_start_time=0, \n",
    "                           sort_epoch_end_time = trial_start_end_sec[-1], \n",
    "                           sort_method = fparams['user_sort_method'])\n",
    "        # finds corresponding interesting roi (roi's to mark with an arrow) order after sorting\n",
    "        interesting_rois = np.in1d(sorted_roi_order, fparams['interesting_rois']).nonzero()[0] \n",
    "else:\n",
    "    sorted_roi_order = range(num_rois)\n",
    "    interesting_rois = fparams['interesting_rois']\n",
    "\n",
    "if not all_nan_rois[0].size == 0:\n",
    "    set_diff_keep_order = lambda main_list, remove_list : [i for i in main_list if i not in remove_list]\n",
    "    sorted_roi_order = set_diff_keep_order(sorted_roi_order, all_nan_rois)\n",
    "    interesting_rois = [i for i in fparams['interesting_rois'] if i not in all_nan_rois]\n",
    "    \n",
    "roi_order_path = os.path.join(fparams['fdir'], fparams['fname'] + '_roi_order.pkl')\n",
    "with open(roi_order_path, 'wb') as handle:\n",
    "     pickle.dump(sorted_roi_order, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trial_avg_heatmap(data_in, conditions, tvec, event_bound_ratio, cmap, clims, sorted_roi_order = None, \n",
    "                           rois_oi = None, save_fig = False, axis_label_size=15):\n",
    "    \n",
    "    \"\"\"\n",
    "    Technically doesn't need to remove all_nan_rois b/c of nanmean calculations\n",
    "    \"\"\"\n",
    "    \n",
    "    num_subplots = len(conditions)\n",
    "    n_columns = np.min([num_subplots, 3.0])\n",
    "    n_rows = int(np.ceil(num_subplots/n_columns))\n",
    "\n",
    "    # set imshow extent to replace x and y axis ticks/labels (replace samples with time)\n",
    "    plot_extent = [tvec[0], tvec[-1], num_rois, 0 ]\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=n_rows, ncols=int(n_columns), figsize = (n_columns*5, n_rows*4))\n",
    "    if not isinstance(ax,np.ndarray): # this is here to make the code below compatible with indexing a single subplot object\n",
    "        ax = [ax]\n",
    "\n",
    "    for idx, cond in enumerate(conditions):\n",
    "\n",
    "        # determine subplot location index\n",
    "        if n_rows == 1:\n",
    "            subplot_index = idx\n",
    "        else:\n",
    "            subplot_index = np.unravel_index(idx, (n_rows, int(n_columns))) # turn int index to a tuple of array coordinates\n",
    "\n",
    "        # prep labels; plot x and y labels for first subplot\n",
    "        if subplot_index == (0, 0) or subplot_index == 0 :\n",
    "            ax[subplot_index].set_ylabel('ROI #', fontsize=axis_label_size)\n",
    "            ax[subplot_index].set_xlabel('Time [s]', fontsize=axis_label_size);\n",
    "        ax[subplot_index].tick_params(axis = 'both', which = 'major', labelsize = tick_font_size)\n",
    "        \n",
    "        # plot the data\n",
    "        if sorted_roi_order is not None:\n",
    "            roi_order = sorted_roi_order\n",
    "        else:\n",
    "            roi_order = slice(0, num_rois)\n",
    "        to_plot = data_in[cond][data_trial_avg_key][roi_order,:] # \n",
    "\n",
    "        im = utils.subplot_heatmap(ax[subplot_index], cond, to_plot, cmap=cmap_, clims=clims, extent_=plot_extent)\n",
    "        ax[subplot_index].axvline(0, color='k', alpha=0.3) # plot vertical line for time zero\n",
    "        ax[subplot_index].annotate('', xy=(event_bound_ratio[0], -0.01), xycoords='axes fraction', \n",
    "                                       xytext=(event_bound_ratio[1], -0.01), \n",
    "                                       arrowprops=dict(arrowstyle=\"-\", color='g'))\n",
    "        if rois_oi is not None:\n",
    "            for ROI_OI in rois_oi:\n",
    "                ax[subplot_index].annotate('', xy=(1.005, 1-(ROI_OI/num_rois)-0.015), xycoords='axes fraction', \n",
    "                                           xytext=(1.06, 1-(ROI_OI/num_rois)-0.015), \n",
    "                                           arrowprops=dict(arrowstyle=\"->\", color='k'))\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    cbar = fig.colorbar(im, ax = ax, shrink = 0.7)\n",
    "    cbar.ax.set_ylabel(ylabel, fontsize=13)\n",
    "    \n",
    "    # hide empty subplot\n",
    "    for a in ax.flat[num_subplots:]:\n",
    "        a.axis('off')\n",
    "    \n",
    "    if save_fig:\n",
    "        fig.savefig(os.path.join(save_dir,'trial_avg_heatmap.png')); \n",
    "        fig.savefig(os.path.join(save_dir,'trial_avg_heatmap.pdf'));\n",
    "\n",
    "plot_trial_avg_heatmap(data_dict, conditions, tvec, event_bound_ratio, cmap_,\n",
    "                       clims = generate_clims(np.concatenate([data_dict[cond][data_trial_avg_key].flatten() for cond in conditions]), \n",
    "                                              fparams['flag_normalization']),\n",
    "                       sorted_roi_order = sorted_roi_order, rois_oi = interesting_rois, save_fig = fparams['flag_save_figs'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot trial- and ROI-averaged traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_shades = []\n",
    "fig, axs = plt.subplots(1,1, figsize = (10,6))\n",
    "for idx, cond in enumerate(conditions):\n",
    "    line_color = cmap_lines(idx)\n",
    "    # first trial avg the data\n",
    "    trial_avg = np.nanmean(data_dict[cond]['zdata'], axis=0)\n",
    "    \n",
    "    # z-score trial-avg data for each respective ROI\n",
    "    # apply zscore function to each row of data\n",
    "    app_axis = 1 \n",
    "    zscore_trial_avg = np.apply_along_axis(utils.zscore_, app_axis, trial_avg, baseline_svec)\n",
    "    \n",
    "    # take avg/std across ROIs\n",
    "    zscore_roi_trial_avg = np.nanmean(zscore_trial_avg, axis=0)\n",
    "    zscore_roi_trial_std = np.nanstd(zscore_trial_avg, axis=0)\n",
    "     \n",
    "    to_plot = np.squeeze(zscore_roi_trial_avg)\n",
    "    to_plot_err = np.squeeze(zscore_roi_trial_std)/np.sqrt(num_rois)\n",
    "    \n",
    "    axs.plot(tvec, to_plot, color=line_color)\n",
    "    if fparams['opto_blank_frame']:\n",
    "        line = axs.plot(tvec[t0_sample:event_end_sample], to_plot[t0_sample:event_end_sample], marker='.', color=line_color)\n",
    "    else:\n",
    "        line = axs.plot(tvec[t0_sample:event_end_sample], to_plot[t0_sample:event_end_sample], color=line_color)\n",
    "    \n",
    "    if fparams['flag_roi_trial_avg_errbar']:\n",
    "        shade = axs.fill_between(tvec, to_plot - to_plot_err, to_plot + to_plot_err, color = line_color,\n",
    "                     alpha=0.2) # this plots the shaded error bar\n",
    "        line_shades.append((line[0],shade))\n",
    "            \n",
    "axs.set_ylabel(ylabel, fontsize=axis_label_size)\n",
    "axs.set_xlabel('Time [s]', fontsize=axis_label_size);\n",
    "axs.legend(conditions);\n",
    "axs.legend(line_shades, conditions, fontsize=15)\n",
    "axs.axvline(0, color='0.5', alpha=0.65) # plot vertical line for time zero\n",
    "axs.annotate('', xy=(event_bound_ratio[0], -0.01), xycoords='axes fraction', \n",
    "                               xytext=(event_bound_ratio[1], -0.01), \n",
    "                               arrowprops=dict(arrowstyle=\"-\", color='g'))\n",
    "axs.tick_params(axis = 'both', which = 'major', labelsize = tick_font_size+3)\n",
    "axs.autoscale(enable=True, axis='both', tight=True)\n",
    "\n",
    "#axs.set_ylim([-1.5, 10])\n",
    "\n",
    "if fparams['flag_save_figs']:\n",
    "        fig.savefig(os.path.join(save_dir,'roi_trial_avg_trace.png')); fig.savefig(os.path.join(save_dir,'roi_trial_avg_trace.pdf'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for finding the index of the closest entry in an array to a provided value\n",
    "def find_nearest_idx(array, value):\n",
    "\n",
    "    if isinstance(array, pd.Series):\n",
    "        idx = (np.abs(array - value)).idxmin()\n",
    "        return idx, array.index.get_loc(idx), array[idx] # series index, 0-relative index, entry value\n",
    "    else:\n",
    "        array = np.asarray(array)\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return idx, array[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quantification of roi-, trial-, time-averaged data\n",
    "\n",
    "analysis_window = fparams['event_sort_analysis_win']\n",
    "analysis_win_samps = [ find_nearest_idx(tvec, time)[0] for time in analysis_window ]\n",
    "\n",
    "to_plot = []\n",
    "to_plot_err = []\n",
    "\n",
    "fig, axs = plt.subplots(1,1, figsize = (5,5))\n",
    "for idx, cond in enumerate(conditions):\n",
    "    line_color = cmap_lines(idx)\n",
    "    # first trial avg the data\n",
    "    trial_avg = np.nanmean(data_dict[cond]['zdata'], axis=0)\n",
    "    \n",
    "    # z-score trial-avg data for each respective ROI\n",
    "    # apply zscore function to each row of data\n",
    "    apply_axis = 1 \n",
    "    zscore_trial_avg = np.apply_along_axis(utils.zscore_, apply_axis, trial_avg, baseline_svec)\n",
    "    \n",
    "    # take avg across time\n",
    "    zscore_trial_time_avg = np.nanmean(zscore_trial_avg[:,analysis_win_samps[0]:analysis_win_samps[1],:], axis=1)\n",
    "    \n",
    "    # take avg/std across ROIs\n",
    "    zscore_roi_trial_time_avg = np.nanmean(zscore_trial_time_avg, axis=0)\n",
    "    zscore_roi_trial_time_std = np.nanstd(zscore_trial_time_avg, axis=0)\n",
    "     \n",
    "    to_plot.append(zscore_roi_trial_time_avg[0])\n",
    "    to_plot_err.append(zscore_roi_trial_time_std[0]/np.sqrt(len(zscore_trial_time_avg)))\n",
    "    \n",
    "barlist = axs.bar(conditions, to_plot, yerr=to_plot_err, align='center', alpha=0.5, ecolor='black', capsize=10 )\n",
    "for idx in range(len(conditions)):\n",
    "    barlist[idx].set_color(cmap_lines(idx))\n",
    "axs.set_ylabel('Normalized Fluorescence', fontsize=13)\n",
    "axs.set_title('ROI-, Trial-, Time-averaged Quant', fontsize=15)\n",
    "axs.yaxis.grid(True)\n",
    "axs.tick_params(axis = 'both', which = 'major', labelsize = tick_font_size)\n",
    "axs.tick_params(axis = 'x', which = 'major', rotation = 45)\n",
    "# Save the figure and show\n",
    "plt.tight_layout()\n",
    "\n",
    "if fparams['flag_save_figs']:\n",
    "    fig.savefig(os.path.join(save_dir,'roi_trial_time_avg_bar.png')); \n",
    "    fig.savefig(os.path.join(save_dir,'roi_trial_time_avg_bar.pdf'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
