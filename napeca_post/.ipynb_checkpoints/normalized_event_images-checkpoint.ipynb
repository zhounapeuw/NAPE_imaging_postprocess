{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "plt.rcParams['text.usetex'] = False\n",
    "plt.rcParams['text.latex.unicode'] = False\n",
    "import matplotlib\n",
    "# important for text to be detecting when importing saved figures into illustrator\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "\n",
    "import s2p_plot_rois_activity_funcs\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER DEFINED VARIABLES\n",
    "fdir = r'D:\\bruker_data\\vj_ofc_imageactivate_02_200\\vj_ofc_imageactivate_02_200_006' # NOTE: the root folder name must match the basename of the _sima_masks.npy file\n",
    "fname_events = 'framenumberforevents_vj_ofc_imageactivate_02_200_006.pickle'\n",
    "preprocess_mode = 'sima' # 's2p' or 'napeca'\n",
    "fs = 30\n",
    "\n",
    "trial_start_end = [-2, 8] # [start, end] times (in seconds) included in the visualization \n",
    "flag_normalization = 'zscore' # options: 'zscore', None\n",
    "baseline_window = [-2, 0]\n",
    "viz_window = [0, 5]\n",
    "\"\"\"\n",
    "define number of ROIs to visualize\n",
    "\n",
    "can be: \n",
    "1) a list of select rois, \n",
    "2) an integer (n) indicating n first rois to plot, or \n",
    "3) None or 'all' which plots all valid ROIs\n",
    "\"\"\" \n",
    "rois_to_plot = None # np.arange(5) #[0,2,3,6] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "fname = os.path.split(fdir)[-1]\n",
    "if 's2p' in preprocess_mode:\n",
    "    path_dict = s2p_plot_rois_activity_funcs.s2p_dir(fdir)\n",
    "    path_dict = s2p_plot_rois_activity_funcs.define_paths_roi_plots(path_dict, None, None, None)\n",
    "elif any(x in preprocess_mode for x in ['sima', 'napeca']):\n",
    "    roi_mask_path = os.path.join(fdir, f'{fname}_sima_masks.npy')\n",
    "    sima_h5_path = os.path.join(fdir, f'{fname}_sima_mc.h5')\n",
    "    \n",
    "events_file_path = os.path.join(fdir, fname_events)\n",
    "    \n",
    "fig_save_dir = os.path.join(fdir, 'figs')\n",
    "if not os.path.exists(fig_save_dir):\n",
    "    os.mkdir(fig_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load projection image\n",
    "proj_manual = {}\n",
    "\n",
    "if any(x in preprocess_mode for x in ['s2p']):\n",
    "    s2p_data_dict = s2p_plot_rois_activity_funcs.load_s2p_data_roi_plots(path_dict)\n",
    "    plot_vars = s2p_plot_rois_activity_funcs.plotting_rois(s2p_data_dict, path_dict)\n",
    "    proj_manual[f'{proj_type}_img'] = s2p_data_dict['ops']['meanImg']\n",
    "\n",
    "elif any(x in preprocess_mode for x in ['sima', 'napeca']):\n",
    "    # load video data\n",
    "    # open h5 to read, find data key, grab data, then close\n",
    "    h5 = h5py.File(sima_h5_path,'r')\n",
    "    data = np.squeeze(np.array( h5[list(h5.keys())[0]] )).astype('int16') # np.array loads all data into memory\n",
    "    h5.close()\n",
    "\n",
    "    proj_manual = {'mean_img': np.mean(data, axis = 0), \n",
    "                   'max_img': np.max(data, axis = 0), \n",
    "                   'std_img': np.std(data, axis = 0) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_event_files = glob.glob(events_file_path) # look for a file in specified directory\n",
    "if not glob_event_files:\n",
    "    print(f'{events_file_path} not detected. Please check if path is correct.')\n",
    "if 'csv' in glob_event_files[0]:\n",
    "    event_times = utils.df_to_dict(glob_event_files[0])\n",
    "elif any(x in glob_event_files[0] for x in ['pkl', 'pickle']):\n",
    "    event_times = pickle.load( open( glob_event_files[0], \"rb\" ), fix_imports=True, encoding='latin1' ) # latin1 b/c original pickle made in python 2\n",
    "event_frames = event_times # utils.dict_time_to_samples(event_times, fs)\n",
    "\n",
    "# identify conditions to analyze\n",
    "all_conditions = event_frames.keys()\n",
    "conditions = [ condition for condition in all_conditions if len(event_frames[condition]) > 0 ] # keep conditions that have events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create variables that reference samples and times for slicing and plotting the data\n",
    "\n",
    "trial_start_end_sec = np.array(trial_start_end) # trial windowing in seconds relative to ttl-onset/trial-onset, in seconds\n",
    "baseline_start_end_sec = np.array(baseline_window)\n",
    "\n",
    "# convert times to samples and get sample vector for the trial \n",
    "trial_begEnd_samp = trial_start_end_sec*fs # turn trial start/end times to samples\n",
    "trial_svec = np.arange(trial_begEnd_samp[0], trial_begEnd_samp[1])\n",
    "# and for baseline period\n",
    "baseline_begEnd_samp = baseline_start_end_sec*fs\n",
    "baseline_svec = (np.arange(baseline_begEnd_samp[0], baseline_begEnd_samp[1]+1, 1) - baseline_begEnd_samp[0]).astype('int')\n",
    "# viz window\n",
    "viz_window_samp = (viz_window-baseline_start_end_sec[0])*fs\n",
    "viz_window_svec = np.arange(viz_window_samp[0], viz_window_samp[1])\n",
    "\n",
    "# calculate time vector for plot x axes\n",
    "num_samples_trial = len( trial_svec )\n",
    "tvec = np.round(np.linspace(trial_start_end_sec[0], trial_start_end_sec[1], num_samples_trial+1), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end_samp = trial_begEnd_samp\n",
    "baseline_start_end_samp = baseline_begEnd_samp\n",
    "\n",
    "# create sample vector for baseline epoch if argument exists (for zscoring)\n",
    "if baseline_begEnd_samp is not None:\n",
    "    baseline_svec = (np.arange(baseline_begEnd_samp[0], baseline_begEnd_samp[1] + 1, 1) -\n",
    "                     baseline_begEnd_samp[0]).astype('int')\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for idx, condition in enumerate(conditions):\n",
    "\n",
    "    data_dict[condition] = {}\n",
    "\n",
    "    # get rid of trials that are outside of the session bounds with respect to time\n",
    "    data_end_sample = data.shape[0]\n",
    "    cond_frame_events = utils.remove_trials_out_of_bounds(data_end_sample, event_frames[condition], start_end_samp[0], start_end_samp[1])\n",
    "\n",
    "    # convert window time bounds to samples and make a trial sample vector\n",
    "    # make an array where the sample indices are repeated in the y axis for n number of trials\n",
    "    num_trials_cond = len(cond_frame_events)\n",
    "    if num_trials_cond == 1:\n",
    "        svec_tile = np.arange(start_end_samp[0], start_end_samp[1] + 1) # just make a 1D vector for svec\n",
    "        num_trial_samps = len(svec_tile)\n",
    "    else:\n",
    "        svec_tile = utils.make_tile(start_end_samp[0], start_end_samp[1], num_trials_cond)\n",
    "        num_trial_samps = svec_tile.shape[1]\n",
    "    \n",
    "    if num_trials_cond > 0:\n",
    "\n",
    "        # now make a repeated matrix of each trial's ttl on sample in the x dimension\n",
    "        ttl_repmat = np.repeat(cond_frame_events[:, np.newaxis], num_trial_samps, axis=1).astype('int')\n",
    "        # calculate actual trial sample indices by adding the TTL onset repeated matrix and the trial window template\n",
    "        trial_sample_mat = np.round(ttl_repmat + svec_tile).astype('int')\n",
    "\n",
    "        # extract frames in trials and reshape the data to be: y,x,trials,samples\n",
    "        # basically unpacking the last 2 dimensions\n",
    "        reshape_dim = (svec_tile.shape) + data.shape[-2:]\n",
    "        extracted_trial_dat = data[np.ndarray.flatten(trial_sample_mat), ...].reshape(reshape_dim)\n",
    "        \n",
    "    # save normalized data\n",
    "    if baseline_start_end_samp is not None:\n",
    "        # input data dimensions should be (trials, ROI, samples)\n",
    "        data_dict[condition]['zdata'] = np.squeeze(np.apply_along_axis(utils.zscore_, 1,\n",
    "                                                                                  extracted_trial_dat,\n",
    "                                                                                  baseline_svec), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "cmap = ListedColormap(sns.color_palette(\"RdBu_r\", 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10,10))\n",
    "im = ax.imshow(np.mean(data_dict[condition]['zdata'][:, viz_window_svec, ...], axis=(0,1)), cmap,\n",
    "               vmin = -2, vmax = 2)\n",
    "\n",
    "cbar = fig.colorbar(im, ax = ax, shrink = 0.5)\n",
    "cbar.ax.set_ylabel('Norm Fluorescence', fontsize = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def parallel_apply_along_axis(func1d, axis, arr, *args, **kwargs):\n",
    "#     \"\"\"\n",
    "#     Like numpy.apply_along_axis(), but takes advantage of multiple\n",
    "#     cores.\n",
    "#     \"\"\"        \n",
    "#     # Effective axis where apply_along_axis() will be applied by each\n",
    "#     # worker (any non-zero axis number would work, so as to allow the use\n",
    "#     # of `np.array_split()`, which is only done on axis 0):\n",
    "#     effective_axis = 1 if axis == 0 else axis\n",
    "#     if effective_axis != axis:\n",
    "#         arr = arr.swapaxes(axis, effective_axis)\n",
    "\n",
    "#     # Chunks for the mapping (only a few chunks):\n",
    "#     chunks = [(func1d, effective_axis, sub_arr, args, kwargs)\n",
    "#               for sub_arr in np.array_split(arr, multiprocessing.cpu_count())]\n",
    "\n",
    "#     pool = multiprocessing.Pool()\n",
    "#     individual_results = pool.map(unpacking_apply_along_axis, chunks)\n",
    "#     # Freeing the workers:\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "\n",
    "#     return np.concatenate(individual_results)\n",
    "\n",
    "# def unpacking_apply_along_axis(all_args):\n",
    "#     (func1d, axis, arr, args, kwargs) = all_args\n",
    "    \n",
    "# tmp = parallel_apply_along_axis(utils.zscore_, 1, extracted_trial_dat, baseline_svec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
