{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ROI Contours and Corresponding Whole-Session Activity Traces\n",
    "\n",
    "What does this script do\n",
    "------------------------------------\n",
    "\n",
    "Generates an interactive plot where the user can select which ROI's whole-session activity is visualized. Event occurrences (ie. timing of events during the session) are denoted by dotted vertical lines and are colored based on condition type. The activity traces and event lines can be toggled on/off and the user can zoom in and out of the plot.\n",
    "\n",
    "How to run this code\n",
    "------------------------------------\n",
    "\n",
    "In this jupyter notebook, First find the code block with the comment header called USER-DEFINED VARIABLES. Edit the variables according to your data and output preferences. Then just run all cells in order (shift + enter; or in the dropdown menu: Kernel->Resart & Run All). Please make sure you set the correct sampling rate (fs) in the user-defined variables block of code.\n",
    "\n",
    "Parameters\n",
    "------------------------------------\n",
    "\n",
    "fname_signal : string\n",
    "    \n",
    "    Name of file that contains roi activity traces. Must include full file name with extension. Accepted file types: .npy, .csv. IMPORTANT: data dimensions should be rois (y) by samples/time (x)\n",
    "\n",
    "fname_events : string\n",
    "\n",
    "    Name of file that contains event occurrences. Must include full file name with extension. Accepted file types: .pkl, .csv. Pickle (pkl) files need to contain a dictionary where keys are the condition names and the values are lists containing samples/frames for each corresponding event. Csv's should have two columns (event condition, sample). The first row are the column names. Subsequent rows contain each trial's event condition and sample in tidy format. See example in sample_data folder for formatting, or this link: https://github.com/zhounapeuw/NAPE_imaging_postprocess/raw/main/docs/_images/napeca_post_event_csv_format.png\n",
    "\n",
    "fdir : string \n",
    "\n",
    "    Root file directory containing the raw tif, tiff, h5 files. IMPORTANT Note: leave off the last backslash, and include the letter r in front of string (treats the contents as a raw string). For example: r'C:\\Users\\my_user\\analyze_sessions'\n",
    "\n",
    "fname : string\n",
    "\n",
    "    Session name; by default this is the name of the parent folder that the data resides in, but can be changed by user to be any string. This fname variable is mainly used to name the saved output files.\n",
    "\n",
    "fs : float\n",
    "\n",
    "    Sampling rate of data. It is imperative that this value is correct; otherwise the incorrect time windows will be pulled out for each event. If you suspect that \n",
    "    the sampling rate (fs) was not set correctly in the NAPECA preprocessing pipeline, go into the saved json file and edit the fs value.\n",
    "\n",
    "opto_blank_frame : boolean\n",
    "\n",
    "    if PMTs were blanked during stim, use detected stim times (from preprocessing) to set those frames to NaN\n",
    "    \n",
    "num_rois : int or string\n",
    "\n",
    "    Set to an (n) integer to plot n first ROIs. Can set to 'all' if want to show all ROIs\n",
    "\n",
    "selected_conditions : list of strings or None\n",
    "\n",
    "    Specific conditions that the user wants to analyze; needs to be exactly the name of conditions in the events CSV or pickle file\n",
    "\n",
    "flag_normalization : string or None\n",
    "\n",
    "    options: 'dff' (delta F over F), 'zscore', 'dff_perc' (delta F/F with baseline F calculated as 25 percentile of whole session's activity. If set to None, no normalization will occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "#important for text to be detected when importing saved figures into illustrator\n",
    "matplotlib.rcParams['pdf.fonttype']=42\n",
    "matplotlib.rcParams['ps.fonttype']=42\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "USER-DEFINED VARIABLES\n",
    "\"\"\"\n",
    "\n",
    "def define_params(method = 'single'):\n",
    "    \n",
    "    fparams = {}\n",
    "    \n",
    "    if method == 'single':\n",
    "        \n",
    "        fparams['fname_signal'] = 'VJ_OFCVTA_7_260_D6_neuropil_corrected_signals_15_50_beta_0.8.csv'   # \n",
    "        fparams['fname_events'] = 'event_times_VJ_OFCVTA_7_260_D6_trained.csv' # can set to None if you want to plot the signals only\n",
    "        # fdir signifies to the root path of the data. Currently, the abspath phrase points to sample data from the repo.\n",
    "        # To specify a path that is on your local computer, use this string format: r'your_root_path', where you should copy/paste\n",
    "        # your path between the single quotes (important to keep the r to render as a complete raw string). See example below:\n",
    "        # r'C:\\Users\\stuberadmin\\Documents\\GitHub\\NAPE_imaging_postprocess\\napeca_post\\sample_data' \n",
    "        fparams['fdir'] = os.path.abspath('./sample_data/VJ_OFCVTA_7_260_D6') \n",
    "        fparams['fname'] = os.path.split(fparams['fdir'])[1]\n",
    "\n",
    "        # set the sampling rate\n",
    "        fparams['fs'] = 5\n",
    "\n",
    "        # session info\n",
    "        fparams['opto_blank_frame'] = False # if PMTs were blanked during stim, set stim times to nan (instead of 0)\n",
    "        \n",
    "        # analysis and plotting arguments\n",
    "        fparams['num_rois'] = 10 # set to 'all' if want to show all cells\n",
    "        fparams['selected_conditions'] = None # set to None if want to include all conditions from behav data\n",
    "        fparams['flag_normalization'] = 'dff_perc' # options: 'dff', 'zscore', 'dff_perc', None\n",
    "        fparams['cond_colors'] = ['steelblue', 'crimson', 'orchid', 'gold']\n",
    "       \n",
    "    elif method == 'f2a': # if string is empty, load predefined list of files in files_to_analyze_event\n",
    "\n",
    "        fparams = files_to_analyze_event.define_fparams()\n",
    "\n",
    "    elif method == 'root_dir':\n",
    "        \n",
    "        pass\n",
    "\n",
    "    return fparams\n",
    "\n",
    "fparams = define_params(method = 'single') # options are 'single', 'f2a', 'root_dir'\n",
    "\n",
    "if fparams['num_rois'] == 'all':\n",
    "    fparams['num_rois'] = signals.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to normalize traces\n",
    "def calc_dff_percentile(activity_vec, perc=25):\n",
    "    perc_activity = np.percentile(activity_vec, perc)\n",
    "    return (activity_vec-perc_activity)/perc_activity\n",
    "\n",
    "def calc_zscore(activity_vec, baseline_samples):\n",
    "    mean_baseline = np.nanmean(data[..., baseline_samples])\n",
    "    std_baseline = np.nanstd(data[..., baseline_samples])\n",
    "    return (data-mean_baseline)/std_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_paths(dict_vars, fparams):\n",
    "    # define load and save paths\n",
    "    dict_vars['signals_fpath'] = os.path.join(fparams['fdir'], fparams['fname_signal'])\n",
    "    if fparams['fname_events']:\n",
    "        dict_vars['events_file_path'] = os.path.join(fparams['fdir'], fparams['fname_events'])\n",
    "\n",
    "    dict_vars['save_dir']= os.path.join(fparams['fdir'], 'event_rel_analysis')\n",
    "    utils.check_exist_dir(dict_vars['save_dir']); # make the save directory\n",
    "\n",
    "    return dict_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_normalize_signals(dict_vars, fparams):\n",
    "    # load time-series data\n",
    "    signals = utils.load_signals(dict_vars['signals_fpath'])\n",
    "\n",
    "    if fparams['opto_blank_frame']:\n",
    "        try:\n",
    "            glob_stim_files = glob.glob(os.path.join(fparams['fdir'], \"{}*_stimmed_frames.pkl\".format(fparams['fname'])))\n",
    "            stim_frames = pickle.load( open( glob_stim_files[0], \"rb\" ) )\n",
    "            signals[:,stim_frames['samples']] = None # blank out stimmed frames\n",
    "            flag_stim = True\n",
    "            print('Detected stim data; replaced stim samples with NaNs')\n",
    "        except:\n",
    "            flag_stim = False\n",
    "            print('Note: No stim preprocessed meta data detected.')\n",
    "\n",
    "    if fparams['flag_normalization'] == 'dff':\n",
    "        dict_vars['signal_to_plot'] = np.apply_along_axis(utils.calc_dff, 1, signals)\n",
    "    elif fparams['flag_normalization'] == 'dff_perc':\n",
    "        dict_vars['signal_to_plot'] = np.apply_along_axis(calc_dff_percentile, 1, signals)\n",
    "    elif fparams['flag_normalization'] == 'zscore':\n",
    "        dict_vars['signal_to_plot'] = np.apply_along_axis(calc_zscore, 1, signals, np.arange(0, signals.shape[1]))\n",
    "    else:\n",
    "        dict_vars['signal_to_plot'] = signals\n",
    "\n",
    "    return dict_vars\n",
    "\n",
    "def calc_min_max(signals):\n",
    "    min_max = [list(min_max_tup) for min_max_tup in zip(np.min(signals,axis=1), np.max(signals,axis=1))]\n",
    "    min_max_all = [np.min(signals), np.max(signals)]\n",
    "    \n",
    "    return min_max_all\n",
    "\n",
    "\n",
    "def make_timing_info(dict_vars, fparams, signals):\n",
    "\n",
    "    dict_vars['total_session_time'] = signals.shape[1]/fparams['fs']\n",
    "    dict_vars['tvec'] = np.round(np.linspace(0, dict_vars['total_session_time'], signals.shape[1]), 2)\n",
    "    \n",
    "    return dict_vars\n",
    "\n",
    "\n",
    "\n",
    "def load_behav(dict_vars, fparams):\n",
    "    ### load behavioral data and trial info\n",
    "    # requires define_paths method to be run first \n",
    "\n",
    "    glob_event_files = glob.glob(dict_vars['events_file_path']) # look for a file in specified directory\n",
    "    if not glob_event_files:\n",
    "        print('{} not detected. Please check if path is correct.'.format(dict_vars['events_file_path']))\n",
    "    if 'csv' in glob_event_files[0]:\n",
    "        dict_vars['event_times'] = utils.df_to_dict(glob_event_files[0])\n",
    "    elif 'pkl' in glob_event_files[0]:\n",
    "        dict_vars['event_times'] = pickle.load( open( glob_event_files[0], \"rb\" ), fix_imports=True, encoding='latin1' ) # latin1 b/c original pickle made in python 2\n",
    "\n",
    "    # identify conditions to analyze\n",
    "    all_conditions = dict_vars['event_times'].keys()\n",
    "    conditions = [ condition for condition in all_conditions if len(dict_vars['event_times'][condition]) > 0 ] # keep conditions that have events\n",
    "\n",
    "    conditions.sort()\n",
    "    if fparams['selected_conditions']:\n",
    "        conditions = fparams['selected_conditions']\n",
    "    dict_vars['conditions'] = conditions\n",
    "    #dict_vars['cmap_lines'] = get_cmap(len(conditions)) # colors for plotting lines for each condition\n",
    "\n",
    "    return dict_vars\n",
    "\n",
    "dict_analysis_vars = {}\n",
    "dict_analysis_vars = define_paths(dict_analysis_vars, fparams)\n",
    "dict_analysis_vars = load_normalize_signals(dict_analysis_vars, fparams)\n",
    "dict_analysis_vars['min_max_all'] = calc_min_max(dict_analysis_vars['signal_to_plot'])\n",
    "dict_analysis_vars = make_timing_info(dict_analysis_vars, fparams, dict_analysis_vars['signal_to_plot'])\n",
    "dict_analysis_vars = load_behav(dict_analysis_vars, fparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "def make_whole_session_plot(dict_vars, fparams):\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces, one for each slider step\n",
    "    for idx_roi in np.arange(fparams['num_rois']):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                visible=False,\n",
    "                name='Activity ({})'.format(fparams['flag_normalization']),\n",
    "                line=dict(color=\"green\", width=1.5),\n",
    "                x=dict_vars['tvec'],\n",
    "                y=dict_vars['signal_to_plot'][idx_roi,:]))\n",
    "\n",
    "    if fparams['fname_events']:\n",
    "        # add vertical lines for events \n",
    "        for idx_cond, cond in enumerate(dict_vars['conditions']):\n",
    "            for idx_ev, event in enumerate(dict_vars['event_times'][cond]):\n",
    "                # 2nd case used to hide trace duplicates in legend\n",
    "                if idx_ev==0:\n",
    "                    fig.add_trace(go.Scatter(x=[event,event],y=[dict_vars['min_max_all'][0],dict_vars['min_max_all'][1]], visible=True,  \n",
    "                                             mode='lines', \n",
    "                                             line=dict(color=fparams['cond_colors'][idx_cond], width=1.5, dash='dash'), showlegend=True, \n",
    "                                             legendgroup=cond,\n",
    "                                             name='{}'.format(cond)))\n",
    "                else:\n",
    "                    fig.add_trace(go.Scatter(x=[event,event], y=[dict_vars['min_max_all'][0],dict_vars['min_max_all'][1]], visible=True,\n",
    "                                             mode='lines',  \n",
    "                                             showlegend=False, legendgroup=cond,\n",
    "                                             line=dict(color=fparams['cond_colors'][idx_cond], width=1.5, dash='dash'),\n",
    "                                             name='{} {}'.format(cond, str(idx_ev))))\n",
    "\n",
    "\n",
    "    # make a list of attributes for slider steps\n",
    "    steps = []\n",
    "    for iroi in np.arange(fparams['num_rois']): \n",
    "        # when ever slider changes, set all roi's to be invisible and all event lines as visible\n",
    "        step = dict(\n",
    "            method=\"restyle\",\n",
    "            # layout attributes\n",
    "            args=[{\"visible\": ([False] * fparams['num_rois']) + [True] * (len(fig.data)- fparams['num_rois'])},\n",
    "                  {\"title\": \"Viewing ROI \" + str(iroi)},\n",
    "                  ])\n",
    "        # then make the selected ROI visible\n",
    "        step[\"args\"][0][\"visible\"][iroi] = True  # Toggle i'th trace to \"visible\"\n",
    "        steps.append(step)\n",
    "\n",
    "    # create and setup slider \n",
    "    sliders = [dict(\n",
    "        active=0,\n",
    "        currentvalue={\"prefix\": \"Viewing \"},\n",
    "        pad={\"t\": 50},\n",
    "        steps=steps\n",
    "    )]\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Time (s)\",\n",
    "        yaxis_title=\"Fluorescence ({})\".format(fparams['flag_normalization']),\n",
    "        legend_title=\"Legend Title\",\n",
    "        font=dict(\n",
    "            size=12),\n",
    "        sliders=sliders,\n",
    "        showlegend=True,\n",
    "        legend_title_text='Legend',\n",
    "        plot_bgcolor='rgba(0,0,0,0)'\n",
    "    )\n",
    "\n",
    "    # rename slider ticks\n",
    "    for idx in np.arange(fparams['num_rois']):\n",
    "        fig['layout']['sliders'][0]['steps'][idx]['label']='ROI ' + str(idx)\n",
    "\n",
    "    # Make 1st trace visible\n",
    "    fig.data[0].visible = True\n",
    "\n",
    "    # Change grid color and axis colors\n",
    "    fig.update_xaxes(showline=True, linewidth=1.5, linecolor='black'); fig.update_yaxes(showline=True, linewidth=1.5, linecolor='black')\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "    \n",
    "whole_session_fig = make_whole_session_plot(dict_analysis_vars, fparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this only outputs the un-altered original plot in vectorized format\n",
    "whole_session_fig.write_image(os.path.join(dict_analysis_vars['save_dir'], 'whole_session_event.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
