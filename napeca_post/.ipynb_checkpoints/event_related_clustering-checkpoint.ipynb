{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA & Clustering\n",
    "\n",
    "The goal of this code is to input activity time-series data from a neural recording and cluster the cells/rois (samples) based on the neural activity (features). Clustering is performed on trial-averaged event-related responses; data from different trial conditions are concatenated and fed into dimensionality reduction (PCA) and finally into multiple clustering algorithms. The optimal hyperparameters for PCA and clustering methods are automatically determined based on the best silhouette score. \n",
    "\n",
    "1) PCA to reduce dimensionality of trial-averaged event-related responses (rois x time), with respect to time dimension. Intuitive concept: PCA is performed on the time dimension (each time point is treated as a feature/variable. That means the resulting principal components (PCs) are linear combinations of the original time points. The first PCs represent each ROI's datapoint resides in n dimensional space where n is the number of samples in the event-related window. PCA finds new set of (orthogonal) axes that maximizes the variance in the activity. These new axes are linear combinations of the original axes\n",
    " \n",
    "\n",
    "2) Clustering: The roi data are now characterized by a reduced set of optimized axes describing time. We now cluster using either kMeans clustering or spectral clustering.\n",
    "    \n",
    "    1. KMeans clustering: Assuming data clouds are gaussian. The three main steps of kMeans clustering are **A)** Initialize the K value, **B)** Calculate the distance between test input and K trained nearest neighbors, **C)** Return class category by taking the majority of votes\n",
    "    \n",
    "    2. Spectral clustering: Not assuming any particular shape of the cluster data points. The three main steps of spectral clustering are **A)** create graph theory similarity matrix for each ROI based on how close other ROIs are in the PCA space, **B)** perform eigendecomposition of the similarity matrix, **C)** Use kmeans clustering on the transformed data. \n",
    "\n",
    "Prerequisites\n",
    "------------------------------------\n",
    "\n",
    "All data should reside in a parent folder. This folder's name should be the name of the session and ideally be the same as the base name of the recording file.\n",
    "\n",
    "Data need to be run through the NAPECA event_rel_analysis code in order to generate the event_data_dict.pkl file, which contains event-related activity across different behavioral conditions for all neurons/ROIs.\n",
    "\n",
    "\n",
    "How to run this code\n",
    "------------------------------------\n",
    "\n",
    "In this jupyter notebook, just run all cells in order (shift + enter).\n",
    "\n",
    "__You can indicate specific files and parameters to include in the second cell__\n",
    "\n",
    "Required Packages\n",
    "-----------------\n",
    "Python 3.7, seaborn, matplotlib, pandas, scikit-learn, statsmodels, numpy, h5py\n",
    "\n",
    "Custom code requirements: utils\n",
    "\n",
    "Parameters \n",
    "----------\n",
    "\n",
    "fname_signal : string\n",
    "    \n",
    "    Name of file that contains roi activity traces. Must include full file name with extension. Accepted file types: .npy, .csv. IMPORTANT: data dimensions should be rois (y) by samples/time (x)\n",
    "\n",
    "fname_events : string\n",
    "\n",
    "    Name of file that contains event occurrences. Must include full file name with extension. Accepted file types: .pkl, .csv. Pickle (pkl) files need to contain a dictionary where keys are the condition names and the values are lists containing samples/frames for each corresponding event. Csv's should have two columns (event condition, sample). The first row are the column names. Subsequent rows contain each trial's event condition and sample in tidy format. See example in sample_data folder for formatting, or this link: https://github.com/zhounapeuw/NAPE_imaging_postprocess/raw/main/docs/_images/napeca_post_event_csv_format.png\n",
    "\n",
    "fdir : string \n",
    "\n",
    "    Root file directory containing the raw tif, tiff, h5 files. Note: leave off the last backslash. For example: ../napeca_post/sample_data if clone the repo directly\n",
    "\n",
    "trial_start_end : list of two entries  \n",
    "\n",
    "    Entries can be ints or floats. The first entry is the time in seconds relative to the event/ttl onset for the start of the event analysis window (negative if before the event/ttl onset. The second entry is the time in seconds for the end of the event analysis window. For example if the desired analysis window is 5.5 seconds before event onset and 8 seconds after, `trial_start_end` would be [-5.5, 8].  \n",
    "    \n",
    "baseline_end : int/float  \n",
    "\n",
    "    Time in seconds for the end of the baseline epoch. By default, the baseline epoch start time will be the first entry ot `trial_start_end`. This baseline epoch is used for calculating baseline normalization metrics.\n",
    "\n",
    "event_sort_analysis_win : list with two float entries\n",
    "\n",
    "    Time window [a, b] in seconds during which some visualization calculations will apply to. For example, if the user sets flag_sort_rois to be True, ROIs in heatmaps will be sorted based on the mean activity in the time window between a and b. \n",
    "\n",
    "pca_num_pc_method : 0 or 1 (int)\n",
    "\n",
    "    Method for calculating number of principal components to retain from PCA preprocessing. 0 for bend in scree plot, 1 for num PCs that account for 90% variance.\n",
    "    User should try either method and observe which result fits the experiment. Sometimes may not impact the results.\n",
    "\n",
    "flag_save_figs : boolean  \n",
    "\n",
    "    Set as True to save figures as JPG and vectorized formats. \n",
    "\n",
    "selected_conditions : list of strings\n",
    "\n",
    "    Specific conditions that the user wants to analyze; needs to be exactly the name of conditions in the events CSV or pickle file\n",
    "\n",
    "flag_plot_reward_line: boolean  \n",
    "\n",
    "    If set to True, plot a vertical line for secondary event. Time of vertical line is dictated by the variable second_event_seconds\n",
    "    \n",
    "second_event_seconds: int/float\n",
    "    \n",
    "    Time in seconds (relative to primary event onset) for plotting a vertical dotted line indicating an optional second event occurrence \n",
    "\n",
    "max_n_clusters : integer\n",
    "    \n",
    "    Maximum number of clusters expected for clustering models. As general rule, select this number based on maximum expected number of clusters in the data + ~5. Keep in mind that larger values will increase processing time\n",
    "    \n",
    "possible_n_nearest_neighbors : array of integers\n",
    "    \n",
    "    In spectral clustering, set n_neighbors to n from the range of possible_n_nearest_neighbors for each data point and create connectivity graph (affinity matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, SVR, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, silhouette_score, adjusted_rand_score, silhouette_samples\n",
    "from sklearn.cluster import AgglomerativeClustering, SpectralClustering, KMeans\n",
    "from sklearn.model_selection import KFold, LeaveOneOut, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn import linear_model\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import (ModelDesc, EvalEnvironment, Term, EvalFactor, LookupFactor, dmatrices, INTERCEPT)\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.colorbar as colorbar\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#important for text to be detected when importing saved figures into illustrator\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype']=42\n",
    "matplotlib.rcParams['ps.fonttype']=42\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "USER-DEFINED VARIABLES\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def define_params(method = 'single'):\n",
    "    \n",
    "    fparams = {}\n",
    "    \n",
    "    if method == 'single':\n",
    "        \n",
    "        fparams['fname_signal'] = 'VJ_OFCVTA_7_260_D6_neuropil_corrected_signals_15_50_beta_0.8.npy' # name of your npy or csv file that contains activity signals\n",
    "        fparams['fname_events'] = 'event_times_VJ_OFCVTA_7_260_D6_trained.csv' # name of your pickle or csv file that contains behavioral event times (in seconds)\n",
    "        # fdir signifies to the root path of the data. Currently, the abspath phrase points to sample data from the repo.\n",
    "        # To specify a path that is on your local computer, use this string format: r'your_root_path', where you should copy/paste\n",
    "        # your path between the single quotes (important to keep the r to render as a complete raw string). See example below:\n",
    "        # r'C:\\Users\\stuberadmin\\Documents\\GitHub\\NAPE_imaging_postprocess\\napeca_post\\sample_data' \n",
    "        fparams['fdir'] = os.path.abspath(\"../napeca_post/sample_data/VJ_OFCVTA_7_260_D6\") # for an explicit path, eg. r'C:\\2pData\\Vijay data\\VJ_OFCVTA_7_D8_trained' \n",
    "        fparams['fname'] = os.path.split(fparams['fdir'])[1]\n",
    "        fparams['flag_close_figs_after_save'] = True\n",
    "        fparams['flag_save_figs'] = True\n",
    "\n",
    "        fparams['fs'] = 5 # sampling rate of activity data\n",
    "\n",
    "        # trial extraction info\n",
    "        fparams['trial_start_end'] = [-2, 8] # trial [start, end] times (in seconds); centered on event onset\n",
    "        fparams['baseline_end'] = -0.2 # baseline epoch end time (in seconds) for performing baseline normalization; I set this to -0.2 to be safe I'm not grabbing a sample that includes the event\n",
    "        fparams['event_sort_analysis_win'] = [0, 5] # time window (in seconds)\n",
    "        fparams['flag_normalization'] = 'zscore' # z-score highly recommended as machine learning algorithms prefer normalized data\n",
    "        \n",
    "        # set to 0 to determine number of PCs to where the scree plot bends, 1 for num PCs that account for 90% variance\n",
    "        fparams['pca_num_pc_method'] = 0 \n",
    "\n",
    "        # variables for clustering\n",
    "        fparams['max_n_clusters'] = 10 # from Vijay: Maximum number of clusters expected. This should be based on the number of functional neuron groups you expect + ~3. In your data, \n",
    "        # might be worth increasing this, but it will take more time to run.\n",
    "        '''In spectral clustering: get n nearest neighbors for each data point and create connectivity graph (affinity matrix)'''\n",
    "        fparams['possible_n_nearest_neighbors'] = np.arange(1, 10) #np.array([3,5,10]) # This should be selected for each dataset\n",
    "        # appropriately. When 4813 neurons are present, the above number of nearest neighbors ([30,40,30,50,60]) provides a good sweep of the\n",
    "        # parameter space. But it will need to be changed for other data.\n",
    "\n",
    "        # optional arguments\n",
    "        fparams['selected_conditions'] = None # set to a list of strings if you want to filter specific conditions to analyze; eg. ['plus', 'minus']\n",
    "        fparams['flag_plot_reward_line'] = False # if there's a second event that happens after the main event, it can be indicated if set to True; timing is dictated by the next variables below\n",
    "        fparams['second_event_seconds'] = 1 # time in seconds\n",
    "        fparams['flag_save_figs'] = False # set to true if you want to save plots\n",
    "        fparams['load_savedpca_or_dopca'] = 'dopca' # Select 'dopca' for doing PCA on the data. Select 'savedpca' for loading my previous results\n",
    "\n",
    "        fparams['group_data'] = False\n",
    "        fparams['group_data_conditions'] = ['cs_plus', 'cs_minus']\n",
    "                \n",
    "    return fparams\n",
    "\n",
    "fparams = define_params(method = 'single') # options are 'single', 'f2a', 'root_dir'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tvec_to_dict(tvec):\n",
    "    \"\"\"\n",
    "    Used for relabeling pandas df columns to make lineplot from df\n",
    "    \"\"\"\n",
    "    dict_ = {}\n",
    "    for i in range(len(tvec)):\n",
    "        dict_[i] = tvec[i] \n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_cluster_colors(dict_vars):\n",
    "    dict_vars['colors_for_cluster'] = [[0.933, 0.250, 0.211],\n",
    "                                              [0.941, 0.352, 0.156],\n",
    "                                              [0.964, 0.572, 0.117],\n",
    "                                              [0.980, 0.686, 0.250],\n",
    "                                              [0.545, 0.772, 0.247],\n",
    "                                              [0.215, 0.701, 0.290],\n",
    "                                              [0, 0.576, 0.270],\n",
    "                                              [0, 0.650, 0.611],\n",
    "                                              [0.145, 0.662, 0.878]]\n",
    "    return dict_vars\n",
    "\n",
    "def normalization_vars(dict_vars, fparams):\n",
    "    if 'zscore' in fparams['flag_normalization']:\n",
    "        dict_vars['data_trial_resolved_key'] = 'zdata'\n",
    "        dict_vars['data_trial_avg_key'] = 'ztrial_avg_data'\n",
    "        dict_vars['cmap_'] = None\n",
    "        dict_vars['ylabel'] = 'Z-score Activity'\n",
    "    else:\n",
    "        dict_vars['data_trial_resolved_key'] = 'data'\n",
    "        dict_vars['data_trial_avg_key'] = 'trial_avg_data'\n",
    "        dict_vars['cmap_'] = 'inferno'\n",
    "        dict_vars['ylabel'] = 'Activity'\n",
    "    \n",
    "    return dict_vars\n",
    "\n",
    "def define_paths(dict_vars, fparams):\n",
    "    dict_vars['signals_fpath'] = os.path.join(fparams['fdir'], fparams['fname_signal'])\n",
    "    dict_vars['events_file_path'] = os.path.join(fparams['fdir'], fparams['fname_events'])\n",
    "\n",
    "    dict_vars['save_dir']= os.path.join(fparams['fdir'], 'event_rel_analysis')\n",
    "    utils.check_exist_dir(dict_vars['save_dir']); # make the save directory\n",
    "\n",
    "    return dict_vars\n",
    "\n",
    "def make_timing_info(dict_vars, fparams):\n",
    "    \n",
    "    # convert times to samples \n",
    "    dict_vars['trial_start_end_sec'] = np.array(fparams['trial_start_end']) # trial windowing in seconds relative to ttl-onset/trial-onset, in seconds\n",
    "    dict_vars['trial_begEnd_samp'] = dict_vars['trial_start_end_sec']*fparams['fs'] # turn trial start/end times to samples\n",
    "    # get sample vector for the trial \n",
    "    trial_svec = np.arange(dict_vars['trial_begEnd_samp'][0], dict_vars['trial_begEnd_samp'][1])\n",
    "    # calculate time vector for plot x axes\n",
    "    num_samples_trial = len( trial_svec )\n",
    "    dict_vars['tvec'] = np.round(np.linspace(dict_vars['trial_start_end_sec'][0], dict_vars['trial_start_end_sec'][1], num_samples_trial+1), 2)\n",
    "    \n",
    "    baseline_start_end_sec = np.array([dict_vars['trial_start_end_sec'][0], fparams['baseline_end']])\n",
    "    dict_vars['baseline_begEnd_samp'] = baseline_start_end_sec*fparams['fs']\n",
    "    dict_vars['baseline_svec'] = (np.arange(dict_vars['baseline_begEnd_samp'][0], dict_vars['baseline_begEnd_samp'][1] + 1, 1) -\n",
    "                            dict_vars['baseline_begEnd_samp'][0]).astype('int')\n",
    "    \n",
    "\n",
    "    return dict_vars\n",
    "\n",
    "def load_behav(dict_vars, fparams):\n",
    "    ### load behavioral data and trial info\n",
    "    # requires define_paths method to be run first \n",
    "\n",
    "    glob_event_files = glob.glob(dict_vars['events_file_path']) # look for a file in specified directory\n",
    "    if not glob_event_files:\n",
    "        print('{} not detected. Please check if path is correct.'.format(dict_vars['events_file_path']))\n",
    "    if 'csv' in glob_event_files[0]:\n",
    "        event_times = utils.df_to_dict(glob_event_files[0])\n",
    "    elif 'pkl' in glob_event_files[0]:\n",
    "        event_times = pickle.load( open( glob_event_files[0], \"rb\" ), fix_imports=True, encoding='latin1' ) # latin1 b/c original pickle made in python 2\n",
    "    dict_vars['event_frames'] = utils.dict_samples_to_time(event_times, fparams['fs'])\n",
    "\n",
    "    # identify conditions to analyze\n",
    "    all_conditions = dict_vars['event_frames'].keys()\n",
    "    conditions = [ condition for condition in all_conditions if len(dict_vars['event_frames'][condition]) > 0 ] # keep conditions that have events\n",
    "\n",
    "    conditions.sort()\n",
    "    if fparams['selected_conditions']:\n",
    "        conditions = fparams['selected_conditions']\n",
    "    dict_vars['conditions'] = conditions\n",
    "    #dict_vars['cmap_lines'] = get_cmap(len(conditions)) # colors for plotting lines for each condition\n",
    "\n",
    "    return dict_vars\n",
    "\n",
    "\n",
    "def format_trial_data_for_ml(dict_vars, signals):\n",
    "    \"\"\"\n",
    "    MAIN data processing function to extract event-centered data\n",
    "\n",
    "    extract and save trial data, \n",
    "    saved data are in the event_rel_analysis subfolder, a pickle file that contains the extracted trial data\n",
    "    \"\"\"\n",
    "    data_dict = utils.extract_trial_data(dict_vars, signals, dict_vars['event_frames'], \n",
    "                                         dict_vars['conditions'], save_dir=dict_vars['save_dir'])\n",
    "    \n",
    "    #### concatenate data across trial conditions\n",
    "\n",
    "    # concatenates data across trials in the time axis; populationdata dimentionss are ROI by time (trials are appended)\n",
    "    formatted_data = np.concatenate([data_dict[condition][dict_vars['data_trial_avg_key']] \n",
    "                                     for condition in dict_vars['conditions']], axis=1)\n",
    "\n",
    "    # remove rows with nan values\n",
    "    nan_rows = np.unique(np.where(np.isnan(formatted_data))[0])\n",
    "    if nan_rows.size != 0:\n",
    "        formatted_data = np.delete(formatted_data, obj=nan_rows, axis=0)\n",
    "        print('Some ROIs contain nans in tseries!')\n",
    "        \n",
    "    return formatted_data\n",
    "\n",
    "\n",
    "def sort_rois (dict_vars, fparams, data):\n",
    "    # sortresponse corresponds to an ordering of the neurons based on their average response in the sortwindow\n",
    "    dict_vars['window_size'] = int(data.shape[1]/len(dict_vars['conditions'])) # Total number of frames in a trial window; needed to split processed concatenated data\n",
    "    dict_vars['sortwindow_frames'] = [utils.get_tvec_sample(dict_vars['tvec'], time) for time in fparams['event_sort_analysis_win']] # Sort responses between first lick and 10 seconds.\n",
    "    dict_vars['sortresponse'] = np.argsort(np.mean(data[:,dict_vars['sortwindow_frames'][0]:dict_vars['sortwindow_frames'][1]], axis=1))[::-1]\n",
    "    \n",
    "    return dict_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_silhouette_plot(dict_vars, X, cluster_labels):\n",
    "\n",
    "    \"\"\"\n",
    "    Create silhouette plot for the clusters\n",
    "    \"\"\"\n",
    "    \n",
    "    n_clusters = len(set(cluster_labels))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(4, 4)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax.set_xlim([-0.4, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels, metric='cosine')\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels, metric='cosine')\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = dict_vars['colors_for_cluster'][i]\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.9)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i+1))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax.set_xticks([-0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    \n",
    "def plot_event_data(dict_vars, fparams, data):\n",
    "\n",
    "    fig, axs = plt.subplots(2,len(dict_vars['conditions']),figsize=(3*2,3*2), sharex='all', sharey='row')\n",
    "\n",
    "    # loop through conditions and plot heatmaps of trial-avged activity\n",
    "    for t in range(len(dict_vars['conditions'])):\n",
    "\n",
    "        # set fig panel to plot in\n",
    "        if len(dict_vars['conditions']) == 1:\n",
    "            ax_heat = axs[0]\n",
    "            ax_trace = axs[1]\n",
    "        else:\n",
    "            ax_heat = axs[0,t]\n",
    "            ax_trace = axs[1,t]\n",
    "\n",
    "        im = utils.subplot_heatmap(ax_heat, ' ', \n",
    "                                   data[dict_vars['sortresponse'], t*dict_vars['window_size']: (t+1)*dict_vars['window_size']], \n",
    "                                   clims = [-dict_vars['cmax'], dict_vars['cmax']], \n",
    "                                   extent_= [dict_vars['tvec'][0], dict_vars['tvec'][-1], data.shape[0], 0 ]) # plot_extent: set plot limits as [time_start, time_end, num_rois, 0]\n",
    "\n",
    "        ax_heat.set_title(dict_vars['conditions'][t])\n",
    "        ax_heat.axvline(0, linestyle='--', color='k', linewidth=0.5)   \n",
    "        if fparams['flag_plot_reward_line']:\n",
    "            ax_heat.axvline(fparams['second_event_seconds'], linestyle='--', color='k', linewidth=0.5) \n",
    "\n",
    "        ### roi-avg tseries \n",
    "\n",
    "        mean_ts = np.mean(data[dict_vars['sortresponse'], t*dict_vars['window_size']:(t+1)*dict_vars['window_size']], axis=0)\n",
    "        stderr_ts = np.std(data[dict_vars['sortresponse'], t*dict_vars['window_size']:(t+1)*dict_vars['window_size']], axis=0)/np.sqrt(data.shape[0])\n",
    "        ax_trace.plot(dict_vars['tvec'], mean_ts)\n",
    "        shade = ax_trace.fill_between(dict_vars['tvec'], mean_ts - stderr_ts, mean_ts + stderr_ts, alpha=0.2) # this plots the shaded error bar\n",
    "\n",
    "        ax_trace.axvline(0, linestyle='--', color='k', linewidth=0.5)  \n",
    "        if fparams['flag_plot_reward_line']:\n",
    "            ax_trace.axvline(fparams['second_event_seconds'], linestyle='--', color='k', linewidth=0.5)   \n",
    "        ax_trace.set_xlabel('Time from event (s)')   \n",
    "\n",
    "        # plot labels on first panel only\n",
    "        if t==0:\n",
    "            ax_trace.set_ylabel('Neurons'); ax_trace.set_ylabel(dict_vars['ylabel'])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    cbar = fig.colorbar(im, ax = axs, shrink = 0.7)\n",
    "    cbar.ax.set_ylabel('Heatmap {}'.format(dict_vars['ylabel']), fontsize=13);\n",
    "\n",
    "    if fparams['flag_save_figs']:\n",
    "        fig.savefig(os.path.join(dict_vars['save_dir'], 'trial_avg_pop_responses.pdf'), format='pdf')\n",
    "        fig.savefig(os.path.join(dict_vars['save_dir'], 'trial_avg_pop_responses.png'), format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare paths and names\n",
    "dict_analysis_vars = {}\n",
    "dict_analysis_vars = set_cluster_colors(dict_analysis_vars)\n",
    "dict_analysis_vars = normalization_vars(dict_analysis_vars, fparams)\n",
    "dict_analysis_vars = define_paths(dict_analysis_vars, fparams)\n",
    "dict_analysis_vars = make_timing_info(dict_analysis_vars, fparams)\n",
    "\n",
    "# load extracted traces from ROIs\n",
    "signals = utils.load_signals(dict_analysis_vars['signals_fpath'])\n",
    "\n",
    "if fparams['group_data']:\n",
    "    \n",
    "    dict_analysis_vars['conditions'] = fparams['group_data_conditions']\n",
    "\n",
    "    # this line takes the pre-trial-averaged data and zscores the traces based on baseline period supplied by user\n",
    "    if fparams['flag_normalization']:\n",
    "        populationdata = np.squeeze(np.apply_along_axis(utils.zscore_, -1, signals, dict_analysis_vars['baseline_svec']))\n",
    "    else:\n",
    "        populationdata = np.squeeze(signals)\n",
    "        \n",
    "    num_samples_trial = int(populationdata.shape[-1]/len(fparams['group_data_conditions']))\n",
    "    dict_analysis_vars['tvec'] = np.round(np.linspace(dict_analysis_vars['trial_start_end_sec'][0], dict_analysis_vars['trial_start_end_sec'][1], num_samples_trial), 2)\n",
    "\n",
    "else:\n",
    "\n",
    "    # load behav data from event times and get condition info\n",
    "    dict_analysis_vars = load_behav(dict_analysis_vars, fparams)\n",
    "    # turn trial-based data into trial-averaged concatenated data ready for machine learning\n",
    "    # data normalization (or lack of) is determined by fparams['flag_normalization']\n",
    "    populationdata = format_trial_data_for_ml(dict_analysis_vars, signals)\n",
    "\n",
    "dict_analysis_vars['cmax'] = np.nanmax(np.abs([np.nanmin(populationdata), np.nanmax(populationdata)])) # Maximum colormap value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort ROIs based on activity in time window; used for plotting event data and sorting ROIs after clustering\n",
    "dict_analysis_vars = sort_rois (dict_analysis_vars, fparams, populationdata)\n",
    "plot_event_data(dict_analysis_vars, fparams, populationdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do PCA to reduce dimensionality in the time-domain\n",
    "\n",
    "PCA: A linear algebra-based method to optimize how a set of variables can explain the variability of a dataset. Optimizing: meaning finding a new set of axes (ie. variables) that are linear combinations of the original axes where each new axis attempts to capture the most amount of variability in the data as possible while remaining linearly independent from the other new axes.\n",
    "\n",
    "In this case, we are finding a new linearly independent parameter space that maximizes the explained variance into the top new axes\n",
    "\n",
    "The goal of using PCA here is to reduce the number of variables explaining the data - the subsequent clustering algorithms perform better with fewer variables. One method of determining the number of variables to keep is to include the first n number of variables that amounts to some amount of variance explained in the data. Usually that number is 90% of variance explained as the last 10% is dominated mostly by variables explaining stochastic noise.\n",
    "\n",
    "Another method is to find the number at which the scree plot bends. This is done by simply bending the scree plot around the line joining (1, variance explained by first PC) and (num of PCs, variance explained by the last PC) and finding the  number of components just below the minimum of this rotated plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions to perform PCA and plot visualizations\n",
    "\n",
    "def num_pc_explained_var(explained_var, explained_var_thresh=90):\n",
    "    \"\"\"\n",
    "    Select pcs for those that capture more than threshold amount of variability in the data\n",
    "    \"\"\"\n",
    "    cum_sum = 0\n",
    "    for idx, PC_var in enumerate(explained_var):\n",
    "        cum_sum += PC_var\n",
    "        if cum_sum > explained_var_thresh:\n",
    "            return idx+1\n",
    "\n",
    "# perform PCA across time\n",
    "def do_PCA(dict_vars, fparams, data):\n",
    "\n",
    "    if fparams['load_savedpca_or_dopca'] == 'dopca':\n",
    "        pca = PCA(n_components=min(data.shape[0],data.shape[1]), whiten=True)\n",
    "        pca.fit(data) \n",
    "        with open(os.path.join(fparams['fdir'], 'pcaresults.pickle'), 'wb') as f:\n",
    "            pickle.dump(pca, f)\n",
    "    elif fparams['load_savedpca_or_dopca'] == 'savedpca':\n",
    "        with open(os.path.join(fparams['fdir'], 'pcaresults.pickle'), 'rb') as f:\n",
    "            pca = pickle.load(f)\n",
    "\n",
    "    # pca across time; transformed data: each ROI is now a linear combination of the original time-serie\n",
    "    dict_vars['transformed_data'] = pca.transform(data)\n",
    "\n",
    "    # grab eigenvectors (pca.components_); linear combination of original axes\n",
    "    dict_vars['pca_vectors'] = pca.components_ \n",
    "    dict_vars['pca_explained_variance'] = pca.explained_variance_ratio_ \n",
    "    print('Number of PCs = {}'.format(dict_vars['pca_vectors'].shape[0]))\n",
    "\n",
    "    # calculate where bend in scree plot is\n",
    "    x = 100*pca.explained_variance_ratio_ # eigenvalue ratios\n",
    "    xprime = x - (x[0] + (x[-1]-x[0])/(x.size-1)*np.arange(x.size))\n",
    "\n",
    "    # define number of PCs\n",
    "    num_retained_pcs_scree = np.argmin(xprime)\n",
    "    num_retained_pcs_var = num_pc_explained_var(x, 90)\n",
    "    if fparams['pca_num_pc_method'] == 0:\n",
    "        dict_vars['num_retained_pcs'] = num_retained_pcs_scree\n",
    "    elif fparams['pca_num_pc_method'] == 1:\n",
    "        dict_vars['num_retained_pcs'] = num_retained_pcs_var\n",
    "   \n",
    "    return dict_vars\n",
    "\n",
    "\n",
    "def plot_PCA_scree(dict_vars, fparams):\n",
    "    # plot PCA scree plot\n",
    "    fig, ax = plt.subplots(figsize=(2,2))\n",
    "    ax.plot(np.arange(dict_vars['pca_explained_variance'].shape[0]).astype(int)+1, dict_vars['pca_explained_variance']*100, 'k')\n",
    "    ax.set_ylabel('Percentage of\\nvariance explained')\n",
    "    ax.set_xlabel('PC number')\n",
    "    ax.axvline(dict_vars['num_retained_pcs'], linestyle='--', color='k', linewidth=0.5)\n",
    "    ax.set_title('Scree plot')\n",
    "    \n",
    "    [i.set_linewidth(0.5) for i in ax.spines.values()]\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    fig.subplots_adjust(left=0.3)\n",
    "    fig.subplots_adjust(right=0.98)\n",
    "    fig.subplots_adjust(bottom=0.25)\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    \n",
    "    if fparams['flag_save_figs']:\n",
    "        fig.savefig(os.path.join(dict_vars['save_dir'], 'scree_plot.png'), format='png', dpi=300)\n",
    "\n",
    "### plot retained principal components\n",
    "def plot_PCs(dict_vars, fparams):\n",
    "    numcols = 2.0\n",
    "    fig, axs = plt.subplots(int(np.ceil(dict_vars['num_retained_pcs']/numcols)), int(numcols), sharey='all',\n",
    "                            figsize=(2.2*numcols, 2.2*int(np.ceil(dict_vars['num_retained_pcs']/numcols))))\n",
    "    \n",
    "    for pc in range(dict_vars['num_retained_pcs']):\n",
    "        ax = axs.flat[pc]\n",
    "        for k, tempkey in enumerate(dict_vars['conditions']):\n",
    "            ax.plot(dict_vars['tvec'], dict_vars['pca_vectors'][pc, k*dict_vars['window_size']:(k+1)*dict_vars['window_size']],\n",
    "                    label='PC %d: %s'%(pc+1, tempkey))\n",
    "\n",
    "        ax.axvline(0, linestyle='--', color='k', linewidth=1)\n",
    "        ax.set_title(f'PC {pc+1}')\n",
    "        # labels on first panel\n",
    "        if pc == 0:\n",
    "            ax.set_xlabel('Time from cue (s)'); ax.set_ylabel( 'PCA weights')\n",
    "            ax.legend(dict_vars['conditions'])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    # hide empty plot if present\n",
    "    for ax in axs.flat[dict_vars['num_retained_pcs']:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    if fparams['flag_save_figs']:\n",
    "        fig.savefig(os.path.join(fparams['save_dir'], 'PCA.png'), format='png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "       \n",
    "dict_analysis_vars = do_PCA(dict_analysis_vars, fparams, populationdata)       \n",
    "plot_PCA_scree(dict_analysis_vars, fparams)\n",
    "plot_PCs(dict_analysis_vars, fparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions to perform clustering\n",
    "\n",
    "def prep_param_testing(dict_vars, fparams, data):\n",
    "    silhouette_dict = {}\n",
    "    silhouette_dict['num_retained_pcs'] = dict_vars['num_retained_pcs']\n",
    "    silhouette_dict['possible_n_nearest_neighbors'] = fparams['possible_n_nearest_neighbors']\n",
    "    silhouette_dict['shape'] = 'cluster_nn'\n",
    "    max_n_clusters = np.min([fparams['max_n_clusters']+1, int(data.shape[0])])\n",
    "    silhouette_dict['possible_n_clusters'] = np.arange(2, max_n_clusters+1) #This requires a minimum of 2 clusters.\n",
    "    # When the data contain no clusters at all, it will be quite visible when inspecting the two obtained clusters, \n",
    "    # as the responses of the clusters will be quite similar. This will also be visible when plotting the data in\n",
    "    # the reduced dimensionality PC space (done below).\n",
    "    \n",
    "    # initialize variables\n",
    "    silhouette_dict['possible_clustering_models'] = np.array([\"Spectral\", \"Kmeans\"])\n",
    "    silhouette_dict['silhouette_scores'] = np.nan*np.ones((silhouette_dict['possible_n_clusters'].size,\n",
    "                                        silhouette_dict['possible_n_nearest_neighbors'].size,\n",
    "                                        silhouette_dict['possible_clustering_models'].size))\n",
    "\n",
    "    return silhouette_dict\n",
    "\n",
    "\n",
    "def perform_hyperparameterization(silhouette_dict, dict_vars, fparams):\n",
    "    # calculate optimal number of clusters and nearest neighbors using silhouette scores\n",
    "    # loop through iterations of clustering params\n",
    "    for n_clustersidx, n_clusters in enumerate(silhouette_dict['possible_n_clusters']):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0) #tol=toler_options\n",
    "        for nnidx, nn in enumerate(silhouette_dict['possible_n_nearest_neighbors']):\n",
    "            spectral = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', n_neighbors=nn, random_state=0)\n",
    "            models = [spectral,kmeans]\n",
    "            for modelidx,model in enumerate(models):\n",
    "                model.fit(dict_vars['transformed_data'][:,:dict_vars['num_retained_pcs']])\n",
    "                silhouette_dict['silhouette_scores'][n_clustersidx, nnidx, modelidx] = silhouette_score(dict_vars['transformed_data'][:,:dict_vars['num_retained_pcs']],\n",
    "                                                                        model.labels_,\n",
    "                                                                        metric='cosine')\n",
    "                if modelidx == 0:\n",
    "                    print('Done with numclusters = {}, num nearest neighbors = {}: score = {}.3f'.\n",
    "                          format(n_clusters, nn, silhouette_dict['silhouette_scores'][n_clustersidx, nnidx, modelidx]))\n",
    "                else:\n",
    "                    print('Done with numclusters = {}, score = {}.3f'.\n",
    "                         format(n_clusters, silhouette_dict['silhouette_scores'][n_clustersidx, nnidx, modelidx]))\n",
    "    print(silhouette_dict['silhouette_scores'].shape)\n",
    "    print('Done with model fitting')\n",
    "\n",
    "#     with open(os.path.join(dict_vars['save_dir'],'silhouette_scores.pkl'), 'wb') as f:\n",
    "#         pickle.dump(silhouette_dict['silhouette_scores'], f)\n",
    "    \n",
    "    return silhouette_dict\n",
    "\n",
    "\n",
    "\n",
    "def extract_optimal_params(silhouette_dict):\n",
    "    # Identify optimal parameters from the hyperparameter evaluation\n",
    "    temp = np.where(silhouette_dict['silhouette_scores']==np.nanmax(silhouette_dict['silhouette_scores']))\n",
    "\n",
    "    silhouette_dict['best_method'] = silhouette_dict['possible_clustering_models'][temp[2][0]]\n",
    "    silhouette_dict['best_n_clusters'] = silhouette_dict['possible_n_clusters'][temp[0][0]]\n",
    "    \n",
    "    if 'Spectral' in silhouette_dict['best_method']:\n",
    "        silhouette_dict['best_n_nearest_neighbors'] = silhouette_dict['possible_n_nearest_neighbors'][temp[1][0]]\n",
    "        print('Optimal params: {} clusters, {} nearest neighbors, {} PCs, {} clustering, silhouette score {}'.\n",
    "          format(silhouette_dict['best_n_clusters'], \n",
    "                 silhouette_dict['best_n_nearest_neighbors'], \n",
    "                 silhouette_dict['num_retained_pcs'], \n",
    "                 silhouette_dict['best_method'],\n",
    "                np.nanmax(silhouette_dict['silhouette_scores'])))\n",
    "    else:\n",
    "        print('Optimal params: {} clusters, {} PCs, {} clustering, silhouette score {}'.\n",
    "          format(silhouette_dict['best_n_clusters'],\n",
    "                 silhouette_dict['num_retained_pcs'], \n",
    "                 silhouette_dict['best_method'], \n",
    "                 np.nanmax(silhouette_dict['silhouette_scores'])))\n",
    "    \n",
    "    return silhouette_dict\n",
    "\n",
    "\n",
    "def reorder_clusters(data, sort_win_frames, rawlabels):\n",
    "    # Since the clustering labels are arbitrary, I rename the clusters so that the first cluster will have the most\n",
    "    # positive response and the last cluster will have the most negative response.\n",
    "    uniquelabels = list(set(rawlabels))\n",
    "    responses = np.nan*np.ones((len(uniquelabels),))\n",
    "    for l, label in enumerate(uniquelabels):\n",
    "        responses[l] = np.mean(data[rawlabels==label, sort_win_frames[0]:sort_win_frames[1]])\n",
    "    temp = np.argsort(responses).astype(int)[::-1]\n",
    "    temp = np.array([np.where(temp==a)[0][0] for a in uniquelabels])\n",
    "    outputlabels = np.array([temp[a] for a in list(np.digitize(rawlabels, uniquelabels)-1)])\n",
    "    return outputlabels\n",
    "\n",
    "\n",
    "def recluster_optimal_params(dict_vars, fparams, silhouette_dict, data):\n",
    "    # Redo clustering with these optimal parameters\n",
    "    model = None\n",
    "    if silhouette_dict['best_method'] == 'Spectral':\n",
    "        model = SpectralClustering(n_clusters=silhouette_dict['best_n_clusters'],\n",
    "                               affinity='nearest_neighbors',\n",
    "                               n_neighbors=silhouette_dict['best_n_nearest_neighbors'],\n",
    "                               random_state=0)\n",
    "    else:\n",
    "        model = KMeans(n_clusters=silhouette_dict['best_n_clusters'], random_state=0)\n",
    "\n",
    "    model.fit(dict_vars['transformed_data'][:,:silhouette_dict['num_retained_pcs']])\n",
    "\n",
    "    # these are the ROIs cluster belongings!\n",
    "    dict_vars['newlabels'] = reorder_clusters(data, dict_vars['sortwindow_frames'], model.labels_)\n",
    "\n",
    "    # Create a new variable containing all unique cluster labels\n",
    "    dict_vars['uniquelabels'] = list(set(dict_vars['newlabels']))\n",
    "\n",
    "\n",
    "    # Save this optimal clustering model.\n",
    "    # with open(os.path.join(save_dir, 'clusteringmodel.pickle'), 'wb') as f:\n",
    "    #     pickle.dump(model, f)\n",
    "\n",
    "    # np.save(os.path.join(summarydictdir, dt_string+'_'+ clusterkey+'_' + 'spectral_clusterlabels.npy'), newlabels)\n",
    "\n",
    "    return dict_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_dict = prep_param_testing(dict_analysis_vars, fparams, populationdata) # set up variables for hyperparameterization\n",
    "silhouette_dict = perform_hyperparameterization(silhouette_dict, dict_analysis_vars, fparams)\n",
    "silhouette_dict = extract_optimal_params(silhouette_dict)\n",
    "dict_analysis_vars = recluster_optimal_params(dict_analysis_vars, fparams, silhouette_dict, populationdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### functions for cluster plotting\n",
    "\n",
    "def plot_cluster_heatmaps(dict_vars, fparams, data):\n",
    "    \n",
    "    fig, axs = plt.subplots(len(dict_vars['conditions']),len(dict_vars['uniquelabels']),\n",
    "                            figsize=(2*len(dict_vars['uniquelabels']),2*len(dict_vars['conditions'])))\n",
    "    if len(axs.shape) == 1:\n",
    "        axs = np.expand_dims(axs, axis=0)\n",
    "\n",
    "    numroisincluster = np.nan*np.ones((len(dict_vars['uniquelabels']),))\n",
    "\n",
    "    for c, cluster in enumerate(dict_vars['uniquelabels']):\n",
    "        for k, tempkey in enumerate(dict_vars['conditions']):\n",
    "\n",
    "            temp = data[np.where(dict_vars['newlabels']==cluster)[0], k*dict_vars['window_size']:(k+1)*dict_vars['window_size']]\n",
    "            numroisincluster[c] = temp.shape[0]\n",
    "            ax=axs[k, cluster]\n",
    "            sortresponse = np.argsort(np.mean(temp[:,dict_vars['sortwindow_frames'][0]:dict_vars['sortwindow_frames'][1]], axis=1))[::-1]\n",
    "\n",
    "            plot_extent = [dict_vars['tvec'][0], dict_vars['tvec'][-1], len(sortresponse), 0 ]\n",
    "            im = utils.subplot_heatmap(ax, ' ', temp[sortresponse], \n",
    "                                       clims = [-dict_vars['cmax'], dict_vars['cmax']], extent_=plot_extent)\n",
    "\n",
    "            axs[k, cluster].grid(False) \n",
    "            if k!=len(dict_vars['conditions'])-1:\n",
    "\n",
    "                axs[k, cluster].set_xticks([])\n",
    "\n",
    "            axs[k, cluster].set_yticks([])\n",
    "            axs[k, cluster].axvline(0, linestyle='--', color='k', linewidth=0.5)\n",
    "            if fparams['flag_plot_reward_line']:\n",
    "                axs[k, cluster].axvline(second_event_seconds, linestyle='--', color='k', linewidth=0.5)\n",
    "            if cluster==0:\n",
    "                axs[k, 0].set_ylabel('%s'%(tempkey))\n",
    "        axs[0, cluster].set_title('Cluster %d\\n(n=%d)'%(cluster+1, numroisincluster[c]))\n",
    "\n",
    "    fig.text(0.4, 0.15, 'Time from cue (s)', fontsize=12,\n",
    "             horizontalalignment='center', verticalalignment='center', rotation='horizontal')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.1); fig.subplots_adjust(left=0.03); fig.subplots_adjust(right=0.93)\n",
    "    fig.subplots_adjust(bottom=0.2); fig.subplots_adjust(top=0.83)                                    \n",
    "\n",
    "    cbar = fig.colorbar(im, ax = axs, shrink = 0.7)\n",
    "    cbar.ax.set_ylabel('Z-Score Activity', fontsize=13);\n",
    "\n",
    "    if fparams['flag_save_figs']:\n",
    "        plt.savefig(os.path.join(dict_vars['save_dir'], 'cluster_heatmap.png'))\n",
    "        plt.savefig(os.path.join(dict_vars['save_dir'], 'cluster_heatmap.pdf'))\n",
    "\n",
    "        \n",
    "def plot_cluster_traces(dict_vars, fparams, data):\n",
    "    # Plot amount of fluorescence normalized for each cluster by conditions over time\n",
    "    fig, axs = plt.subplots(1,len(dict_vars['uniquelabels']),\n",
    "                            figsize=(3*len(dict_vars['uniquelabels']),1.5*len(dict_vars['conditions'])))\n",
    "\n",
    "    numroisincluster = np.nan*np.ones((len(dict_vars['uniquelabels']),))\n",
    "\n",
    "    for c, cluster in enumerate(dict_vars['uniquelabels']):\n",
    "        for k, tempkey in enumerate(dict_vars['conditions']):\n",
    "            temp = data[np.where(dict_vars['newlabels']==cluster)[0], k*dict_vars['window_size']:(k+1)*dict_vars['window_size']]\n",
    "            numroisincluster[c] = temp.shape[0]\n",
    "            sortresponse = np.argsort(np.mean(temp[:,dict_vars['sortwindow_frames'][0]:dict_vars['sortwindow_frames'][1]], axis=1))[::-1]\n",
    "            sns.lineplot(x=\"variable\", y=\"value\",data = pd.DataFrame(temp[sortresponse]).rename(columns=tvec_to_dict(dict_vars['tvec'])).melt(),\n",
    "                        ax = axs[cluster],\n",
    "                        palette=plt.get_cmap('coolwarm'),label = tempkey,legend = False)\n",
    "            axs[cluster].grid(False)  \n",
    "            axs[cluster].axvline(0, linestyle='--', color='k', linewidth=0.5)\n",
    "            axs[cluster].spines['right'].set_visible(False); axs[cluster].spines['top'].set_visible(False)\n",
    "            if cluster==0:\n",
    "                axs[cluster].set_ylabel('Normalized fluorescence')\n",
    "            else:\n",
    "                axs[cluster].set_ylabel('')\n",
    "            axs[cluster].set_xlabel('')\n",
    "        axs[cluster].set_title('Cluster %d\\n(n=%d)'%(cluster+1, numroisincluster[c]))\n",
    "        axs[0].legend()\n",
    "    fig.text(0.5, 0.05, 'Time from cue (s)', fontsize=12,\n",
    "             horizontalalignment='center', verticalalignment='center', rotation='horizontal')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.1); fig.subplots_adjust(left=0.03); fig.subplots_adjust(right=0.93)\n",
    "    fig.subplots_adjust(bottom=0.2); fig.subplots_adjust(top=0.83)\n",
    "\n",
    "    if fparams['flag_save_figs']:\n",
    "        plt.savefig(os.path.join(dict_vars['save_dir'], 'cluster_roiAvg_traces.png'))\n",
    "        plt.savefig(os.path.join(dict_vars['save_dir'], 'cluster_roiAvg_traces.pdf'))\n",
    "        \n",
    "        \n",
    "def plot_clusters_in_pca_space(dict_vars, fparams, silhouette_dict):\n",
    "    # Perform TSNE on newly defined clusters\n",
    "    num_clusterpairs = len(dict_vars['uniquelabels'])*(len(dict_vars['uniquelabels'])-1)/2\n",
    "    numrows = int(np.ceil(num_clusterpairs**0.5)); numcols = int(np.ceil(num_clusterpairs/np.ceil(num_clusterpairs**0.5)))\n",
    "    fig, axs = plt.subplots(numrows, numcols, figsize=(3*numrows, 3*numcols))\n",
    "\n",
    "    tempsum = 0\n",
    "    for c1, cluster1 in enumerate(dict_vars['uniquelabels']):\n",
    "        for c2, cluster2 in enumerate(dict_vars['uniquelabels']):\n",
    "            if cluster1>=cluster2:\n",
    "                continue\n",
    "\n",
    "            temp1 = dict_vars['transformed_data'][np.where(dict_vars['newlabels']==cluster1)[0], :silhouette_dict['num_retained_pcs']]\n",
    "            temp2 = dict_vars['transformed_data'][np.where(dict_vars['newlabels']==cluster2)[0], :silhouette_dict['num_retained_pcs']]\n",
    "            X = np.concatenate((temp1, temp2), axis=0)\n",
    "\n",
    "            tsne = TSNE(n_components=2, init='random',\n",
    "                        random_state=0, perplexity=np.sqrt(X.shape[0]))\n",
    "            Y = tsne.fit_transform(X)\n",
    "\n",
    "            if numrows*numcols==1:\n",
    "                ax = axs\n",
    "            else:\n",
    "                ax = axs[int(tempsum/numcols),\n",
    "                         abs(tempsum - int(tempsum/numcols)*numcols)]\n",
    "            ax.scatter(Y[:np.sum(dict_vars['newlabels']==cluster1),0],\n",
    "                       Y[:np.sum(dict_vars['newlabels']==cluster1),1],\n",
    "                       color=dict_vars['colors_for_cluster'][cluster1], label='Cluster %d'%(cluster1+1), alpha=1)\n",
    "            ax.scatter(Y[np.sum(dict_vars['newlabels']==cluster1):,0],\n",
    "                       Y[np.sum(dict_vars['newlabels']==cluster1):,1],\n",
    "                       color=dict_vars['colors_for_cluster'][cluster2+3], label='Cluster %d'%(cluster2+1), alpha=1)\n",
    "\n",
    "            ax.set_xlabel('tsne dimension 1')\n",
    "            ax.set_ylabel('tsne dimension 2')\n",
    "            ax.legend()\n",
    "            tempsum += 1\n",
    "\n",
    "            fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_cluster_heatmaps(dict_analysis_vars, fparams, data)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_traces(dict_analysis_vars, fparams, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_in_pca_space(dict_analysis_vars, fparams, silhouette_dict)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
