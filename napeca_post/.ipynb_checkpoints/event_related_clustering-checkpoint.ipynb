{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA & Clustering\n",
    "\n",
    "The goal of this code is to input activity time-series data from a neural recording and cluster the cells/rois (samples) based on the neural activity (features). Clustering is performed on trial-averaged event-related responses; data from different trial conditions are concatenated and fed into dimensionality reduction (PCA) and finally into multiple clustering algorithms. The optimal hyperparameters for PCA and clustering methods are automatically determined based on the best silhouette score. \n",
    "\n",
    "1) PCA to reduce dimensionality of trial-averaged event-related responses (rois x time), with respect to time dimension. Intuitive concept: PCA is performed on the time dimension (each time point is treated as a feature/variable. That means the resulting principal components (PCs) are linear combinations of the original time points. The first PCs represent each ROI's datapoint resides in n dimensional space where n is the number of samples in the event-related window. PCA finds new set of (orthogonal) axes that maximizes the variance in the activity. These new axes are linear combinations of the original axes\n",
    " \n",
    "\n",
    "2) Clustering: The roi data are now characterized by a reduced set of optimized axes describing time. We now cluster using either kMeans clustering or spectral clustering.\n",
    "    \n",
    "    1. KMeans clustering: Assuming data clouds are gaussian. The three main steps of kMeans clustering are **A)** Initialize the K value, **B)** Calculate the distance between test input and K trained nearest neighbors, **C)** Return class category by taking the majority of votes\n",
    "    \n",
    "    2. Spectral clustering: Not assuming any particular shape of the cluster data points. The three main steps of spectral clustering are **A)** create graph theory similarity matrix for each ROI based on how close other ROIs are in the PCA space, **B)** perform eigendecomposition of the similarity matrix, **C)** Use kmeans clustering on the transformed data. \n",
    "\n",
    "Prerequisites\n",
    "------------------------------------\n",
    "\n",
    "All data should reside in a parent folder. This folder's name should be the name of the session and ideally be the same as the base name of the recording file.\n",
    "\n",
    "Data need to be run through the NAPECA event_rel_analysis code in order to generate the event_data_dict.pkl file, which contains event-related activity across different behavioral conditions for all neurons/ROIs.\n",
    "\n",
    "\n",
    "How to run this code\n",
    "------------------------------------\n",
    "\n",
    "In this jupyter notebook, just run all cells in order (shift + enter).\n",
    "\n",
    "__You can indicate specific files and parameters to include in the second cell__\n",
    "\n",
    "Required Packages\n",
    "-----------------\n",
    "Python 3.7, seaborn, matplotlib, pandas, scikit-learn, statsmodels, numpy, h5py\n",
    "\n",
    "Custom code requirements: utils\n",
    "\n",
    "Parameters \n",
    "----------\n",
    "\n",
    "fname_signal : string\n",
    "    \n",
    "    Name of file that contains roi activity traces. Must include full file name with extension. Accepted file types: .npy, .csv. IMPORTANT: data dimensions should be rois (y) by samples/time (x)\n",
    "\n",
    "fname_events : string\n",
    "\n",
    "    Name of file that contains event occurrences. Must include full file name with extension. Accepted file types: .pkl, .csv. Pickle (pkl) files need to contain a dictionary where keys are the condition names and the values are lists containing samples/frames for each corresponding event. Csv's should have two columns (event condition, sample). The first row are the column names. Subsequent rows contain each trial's event condition and sample in tidy format. See example in sample_data folder for formatting, or this link: https://github.com/zhounapeuw/NAPE_imaging_postprocess/raw/main/docs/_images/napeca_post_event_csv_format.png\n",
    "\n",
    "fdir : string \n",
    "\n",
    "    Root file directory containing the raw tif, tiff, h5 files. Note: leave off the last backslash. For example: ../napeca_post/sample_data if clone the repo directly\n",
    "\n",
    "trial_start_end : list of two entries  \n",
    "\n",
    "    Entries can be ints or floats. The first entry is the time in seconds relative to the event/ttl onset for the start of the event analysis window (negative if start time is before the event/ttl onset). Event analysis window refers to the primary time window around events that is visualized in plots. The second entry is the time in seconds for the end of the event analysis window. For example if the desired analysis window is 5.5 seconds before event onset and 8 seconds after, `trial_start_end` would be [-5.5, 8].  \n",
    "    \n",
    "baseline_start_end : list of two entries  \n",
    "\n",
    "    Entries can be ints or floats. The first entry is the time in seconds relative to the event/ttl onset for the start of the baseline window (negative if start time is before the event/ttl onset). Baseline window refers to the time window relative to event onset that is used to calculate the mean and/or standard deviation for normalization. The second entry is the time in seconds (relative to event onset) for the end of the baseline window. For example if the desired analysis window is 5.5 seconds to 0.2 seconds before event onset, `baseline_start_end` would be [-5.5, -0.2]. If a single number is supplied, the baseline window onset will default to the first entry of `trial_start_end` and window end will be the value supplied to `baseline_start_end`.\n",
    "\n",
    "event_sort_analysis_win : list with two float entries\n",
    "\n",
    "    Time window [a, b] in seconds during which some visualization calculations will apply to. For example, if the user sets flag_sort_rois to be True, ROIs in heatmaps will be sorted based on the mean activity in the time window between a and b. \n",
    "\n",
    "pca_num_pc_method : 0 or 1 (int)\n",
    "\n",
    "    Method for calculating number of principal components to retain from PCA preprocessing. 0 for bend in scree plot, 1 for num PCs that account for 90% variance.\n",
    "    User should try either method and observe which result fits the experiment. Sometimes may not impact the results.\n",
    "\n",
    "flag_save_figs : boolean  \n",
    "\n",
    "    Set as True to save figures as JPG and vectorized formats. \n",
    "\n",
    "selected_conditions : list of strings\n",
    "\n",
    "    Specific conditions that the user wants to analyze; needs to be exactly the name of conditions in the events CSV or pickle file\n",
    "\n",
    "flag_plot_reward_line: boolean  \n",
    "\n",
    "    If set to True, plot a vertical line for secondary event. Time of vertical line is dictated by the variable second_event_seconds\n",
    "    \n",
    "second_event_seconds: int/float\n",
    "    \n",
    "    Time in seconds (relative to primary event onset) for plotting a vertical dotted line indicating an optional second event occurrence \n",
    "\n",
    "max_n_clusters : integer\n",
    "    \n",
    "    Maximum number of clusters expected for clustering models. As general rule, select this number based on maximum expected number of clusters in the data + ~5. Keep in mind that larger values will increase processing time\n",
    "    \n",
    "possible_n_nearest_neighbors : array of integers\n",
    "    \n",
    "    In spectral clustering, set n_neighbors to n from the range of possible_n_nearest_neighbors for each data point and create connectivity graph (affinity matrix).\n",
    "    \n",
    "    In general, choosing the value of possible_n_nearest_neighbors is sqrt(N) where N stands for the number of samples in your dataset. ( https://towardsdatascience.com/a-simple-introduction-to-k-nearest-neighbors-algorithm-b3519ed98e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, SVR, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, silhouette_score, adjusted_rand_score, silhouette_samples\n",
    "from sklearn.cluster import AgglomerativeClustering, SpectralClustering, KMeans\n",
    "from sklearn.model_selection import KFold, LeaveOneOut, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn import linear_model\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import (ModelDesc, EvalEnvironment, Term, EvalFactor, LookupFactor, dmatrices, INTERCEPT)\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.colorbar as colorbar\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#important for text to be detected when importing saved figures into illustrator\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype']=42\n",
    "matplotlib.rcParams['ps.fonttype']=42\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "USER-DEFINED VARIABLES\n",
    "\"\"\"\n",
    "\n",
    "fname_signal = 'VJ_OFCVTA_7_260_D6_neuropil_corrected_signals_15_50_beta_0.8.npy'   # name of your npy or csv file that contains activity signals\n",
    "fname_events = 'event_times_VJ_OFCVTA_7_260_D6_trained.csv' # name of your pickle or csv file that contains behavioral event times (in seconds)\n",
    "# fdir signifies to the root path of the data. Currently, the abspath phrase points to sample data from the repo.\n",
    "# To specify a path that is on your local computer, use this string format: r'your_root_path', where you should copy/paste\n",
    "# your path between the single quotes (important to keep the r to render as a complete raw string). See example below:\n",
    "# r'C:\\Users\\stuberadmin\\Documents\\GitHub\\NAPE_imaging_postprocess\\napeca_post\\sample_data' \n",
    "fdir = os.path.abspath('./sample_data/VJ_OFCVTA_7_260_D6') # for an explicit path, eg. r'C:\\2pData\\Vijay data\\VJ_OFCVTA_7_D8_trained'\n",
    "fs = 5 # sampling rate of activity data\n",
    "\n",
    "# trial extraction info\n",
    "trial_start_end = [-2, 8] # primary visualization window relative to event onset; [start, end] times (in seconds) \n",
    "baseline_start_end = [-2, -0.2] # baseline window (in seconds) for performing baseline normalization. either a list [start, end] or an int/float (see details in markdown above); I set this to -0.2 to be safe I'm not grabbing a sample that includes the event\n",
    "event_sort_analysis_win = [0, 5] # time window (in seconds)\n",
    "\n",
    "pca_num_pc_method = 0 # 0 for bend in scree plot, 1 for num PCs that account for 90% variance\n",
    "\n",
    "# variables for clustering\n",
    "max_n_clusters = 10 # from Vijay: Maximum number of clusters expected. This should be based on the number of functional neuron groups you expect + ~3. In your data, \n",
    "# might be worth increasing this, but it will take more time to run.\n",
    "\n",
    "'''In spectral clustering: get n nearest neighbors for each data point and create connectivity graph (affinity matrix)'''\n",
    "possible_n_nearest_neighbors = np.arange(1, 10) #np.array([3,5,10]) # This should be selected for each dataset\n",
    "# appropriately. When 4813 neurons are present, the above number of nearest neighbors ([30,40,30,50,60]) provides a good sweep of the\n",
    "# parameter space. But it will need to be changed for other data.\n",
    "\n",
    "# optional arguments\n",
    "selected_conditions = None # set to a list of strings if you want to filter specific conditions to analyze; eg. ['plus', 'minus']\n",
    "flag_plot_reward_line = False # if there's a second event that happens after the main event, it can be indicated if set to True; timing is dictated by the next variables below\n",
    "second_event_seconds = 1 # time in seconds\n",
    "flag_save_figs = True # set to true if you want to save plots\n",
    "heatmap_cmap_scaling = 1 # set to lower value if colormap range is too large\n",
    "\n",
    "# set to True if the data you are loading in already has data from different conditions concatenated together\n",
    "# do not set to True if data come directly from suite2p or sima/napeca preprocessing!\n",
    "group_data = False\n",
    "group_data_conditions = ['cs_plus', 'cs_minus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare paths and names\n",
    "fname = os.path.split(fdir)[1]\n",
    "signals_fpath = os.path.join(fdir, fname_signal)\n",
    "save_dir = os.path.join(fdir, 'event_rel_analysis')\n",
    "utils.check_exist_dir(save_dir); # make the save directory\n",
    "\n",
    "signals = utils.load_signals(signals_fpath)\n",
    "\n",
    "trial_start_end_sec = np.array(trial_start_end) # trial windowing in seconds relative to ttl-onset/trial-onset, in seconds\n",
    "if type(baseline_start_end) is list:\n",
    "    baseline_start_end_sec = np.array(baseline_start_end)\n",
    "elif isinstance(baseline_start_end, (int, float)):\n",
    "    baseline_start_end_sec = np.array([trial_start_end_sec[0], baseline_start_end])\n",
    "    \n",
    "baseline_begEnd_samp = baseline_start_end_sec*fs\n",
    "baseline_svec = (np.arange(baseline_begEnd_samp[0], baseline_begEnd_samp[1] + 1, 1) -\n",
    "                        baseline_begEnd_samp[0]).astype('int')\n",
    "\n",
    "if group_data:\n",
    "    conditions = group_data_conditions\n",
    "\n",
    "    if selected_conditions:\n",
    "        conditions = selected_conditions\n",
    "\n",
    "    num_conditions = len(conditions)\n",
    "\n",
    "    populationdata = np.squeeze(np.apply_along_axis(utils.zscore_, -1, signals, baseline_svec))\n",
    "\n",
    "    num_samples_trial = int(populationdata.shape[-1]/len(group_data_conditions))\n",
    "    tvec = np.round(np.linspace(trial_start_end_sec[0], trial_start_end_sec[1], num_samples_trial), 2)\n",
    "\n",
    "else:\n",
    "\n",
    "    events_file_path = os.path.join(fdir, fname_events)\n",
    "\n",
    "    \n",
    "    glob_event_files = glob.glob(events_file_path) # look for a file in specified directory\n",
    "    if not glob_event_files:\n",
    "        print(f'{events_file_path} not detected. Please check if path is correct.')\n",
    "    if 'csv' in glob_event_files[0]:\n",
    "        event_times = utils.df_to_dict(glob_event_files[0])\n",
    "    elif any(x in glob_event_files[0] for x in ['pkl', 'pickle']):\n",
    "        event_times = pickle.load( open( glob_event_files[0], \"rb\" ), fix_imports=True, encoding='latin1' ) # latin1 b/c original pickle made in python 2\n",
    "    event_frames = utils.dict_time_to_samples(event_times, fs)\n",
    "\n",
    "\n",
    "    # identify conditions to analyze\n",
    "    all_conditions = event_frames.keys()\n",
    "    conditions = [ condition for condition in all_conditions if len(event_frames[condition]) > 0 ] # keep conditions that have events\n",
    "\n",
    "    conditions.sort()\n",
    "    if selected_conditions:\n",
    "        conditions = selected_conditions\n",
    "\n",
    "    num_conditions = len(conditions)\n",
    "\n",
    "    ### define trial timing\n",
    "\n",
    "    # convert times to samples and get sample vector for the trial \n",
    "    trial_begEnd_samp = trial_start_end_sec*fs # turn trial start/end times to samples\n",
    "    trial_svec = np.arange(trial_begEnd_samp[0], trial_begEnd_samp[1])\n",
    "    # calculate time vector for plot x axes\n",
    "    num_samples_trial = len( trial_svec )\n",
    "    tvec = np.round(np.linspace(trial_start_end_sec[0], trial_start_end_sec[1], num_samples_trial+1), 2)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    MAIN data processing function to extract event-centered data\n",
    "\n",
    "    extract and save trial data, \n",
    "    saved data are in the event_rel_analysis subfolder, a pickle file that contains the extracted trial data\n",
    "    \"\"\"\n",
    "    data_dict = utils.extract_trial_data(signals, tvec, trial_begEnd_samp, event_frames, \n",
    "                                        conditions, baseline_start_end_samp = baseline_begEnd_samp, save_dir=None)\n",
    "\n",
    "\n",
    "    #### concatenate data across trial conditions\n",
    "\n",
    "    # concatenates data across trials in the time axis; populationdata dimentionss are ROI by time (trials are appended)\n",
    "    populationdata = np.concatenate([data_dict[condition]['ztrial_avg_data'] for condition in conditions], axis=1)\n",
    "    np.save(os.path.join(save_dir, 'cluster_pop_data.npy'), populationdata)\n",
    "    \n",
    "    # remove rows with nan values\n",
    "    nan_rows = np.unique(np.where(np.isnan(populationdata))[0])\n",
    "    if nan_rows.size != 0:\n",
    "        populationdata = np.delete(populationdata, obj=nan_rows, axis=0)\n",
    "        print('Some ROIs contain nan in tseries!')\n",
    "\n",
    "cmax = np.nanmax(np.abs([np.nanmin(populationdata), np.nanmax(populationdata)])) # Maximum colormap value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_plot_graphics(ax):\n",
    "    \"\"\"\n",
    "    Standardize plots\n",
    "    \"\"\"\n",
    "    [i.set_linewidth(0.5) for i in ax.spines.itervalues()] # change the width of spines for both axis\n",
    "    ax.spines['right'].set_visible(False) # remove top the right axis\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    return ax\n",
    "\n",
    "def fit_regression(x, y):\n",
    "    \"\"\"\n",
    "    Fit a linear regression with ordinary least squares\n",
    "    \"\"\"\n",
    "    lm = sm.OLS(y, sm.add_constant(x)).fit() # add a column of 1s for intercept before fitting\n",
    "    x_range = sm.add_constant(np.array([x.min(), x.max()]))\n",
    "    x_range_pred = lm.predict(x_range)\n",
    "    return lm.pvalues[1], lm.params[1], x_range[:,1], x_range_pred, lm.rsquared\n",
    "\n",
    "def CDFplot(x, ax, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a cumulative distribution function (CDF) plot\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    ix= np.argsort(x)\n",
    "    ax.plot(x[ix], ECDF(x)(x)[ix], **kwargs)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def fit_regression_and_plot(x, y, ax, plot_label='', color='k', linecolor='r', markersize=3,\n",
    "                            show_pval=True):\n",
    "    \"\"\"\n",
    "    Fit a linear regression model with ordinary least squares and visualize the results\n",
    "    \"\"\"\n",
    "    #linetype is a string like 'bo'\n",
    "    pvalue, slope, temp, temppred, R2 = fit_regression(x, y)   \n",
    "    if show_pval:\n",
    "        plot_label = '%s p=%.2e\\nr=%.3f'% (plot_label, pvalue, np.sign(slope)*np.sqrt(R2))\n",
    "    else:\n",
    "        plot_label = '%s r=%.3f'% (plot_label, np.sign(slope)*np.sqrt(R2))\n",
    "    ax.scatter(x, y, color=color, label=plot_label, s=markersize)\n",
    "    ax.plot(temp, temppred, color=linecolor)\n",
    "    return ax, slope, pvalue, R2\n",
    "\n",
    "\n",
    "def make_silhouette_plot(X, cluster_labels):\n",
    "\n",
    "    \"\"\"\n",
    "    Create silhouette plot for the clusters\n",
    "    \"\"\"\n",
    "    \n",
    "    n_clusters = len(set(cluster_labels))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(4, 4)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax.set_xlim([-0.4, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels, metric='cosine')\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels, metric='cosine')\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = colors_for_cluster[i]\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.9)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i+1))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax.set_xticks([-0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for plotting\n",
    "\n",
    "# calculated variables\n",
    "window_size = int(populationdata.shape[1]/num_conditions) # Total number of frames in a trial window; needed to split processed concatenated data\n",
    "sortwindow_frames = [int(np.round(time*fs)) for time in event_sort_analysis_win] # Sort responses between first lick and 10 seconds.\n",
    "sortresponse = np.argsort(np.mean(populationdata[:,sortwindow_frames[0]:sortwindow_frames[1]], axis=1))[::-1]\n",
    "# sortresponse corresponds to an ordering of the neurons based on their average response in the sortwindow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,num_conditions,figsize=(3*2,3*2), sharex='all', sharey='row')\n",
    "\n",
    "# loop through conditions and plot heatmaps of trial-avged activity\n",
    "for t in range(num_conditions):\n",
    "    \n",
    "    if num_conditions == 1:\n",
    "        ax = axs[0]\n",
    "    else:\n",
    "        ax = axs[0,t]\n",
    "\n",
    "    plot_extent = [tvec[0], tvec[-1], populationdata.shape[0], 0 ] # set plot limits as [time_start, time_end, num_rois, 0]\n",
    "    im = utils.subplot_heatmap(ax, ' ', populationdata[sortresponse, t*window_size: (t+1)*window_size], \n",
    "                               clims = [-cmax, cmax], extent_=plot_extent)\n",
    "    ax.set_title(conditions[t])\n",
    "    \n",
    "    ax.axvline(0, linestyle='--', color='k', linewidth=0.5)   \n",
    "    if flag_plot_reward_line:\n",
    "        ax.axvline(second_event_seconds, linestyle='--', color='k', linewidth=0.5) \n",
    "    \n",
    "    ### roi-avg tseries \n",
    "    if num_conditions == 1:\n",
    "        ax = axs[1]\n",
    "    else:\n",
    "        ax = axs[1,t]\n",
    "    mean_ts = np.mean(populationdata[sortresponse, t*window_size:(t+1)*window_size], axis=0)\n",
    "    stderr_ts = np.std(populationdata[sortresponse, t*window_size:(t+1)*window_size], axis=0)/np.sqrt(populationdata.shape[0])\n",
    "    ax.plot(tvec, mean_ts)\n",
    "    shade = ax.fill_between(tvec, mean_ts - stderr_ts, mean_ts + stderr_ts, alpha=0.2) # this plots the shaded error bar\n",
    "    ax.axvline(0, linestyle='--', color='k', linewidth=0.5)  \n",
    "    if flag_plot_reward_line:\n",
    "        ax.axvline(second_event_seconds, linestyle='--', color='k', linewidth=0.5)   \n",
    "    ax.set_xlabel('Time from event (s)')   \n",
    " \n",
    "    if t==0:\n",
    "        ax.set_ylabel('Neurons')\n",
    "        ax.set_ylabel('Mean norm. fluor.')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "cbar = fig.colorbar(im, ax = axs, shrink = 0.7)\n",
    "cbar.ax.set_ylabel('Heatmap Z-Score Activity', fontsize=13);\n",
    "\n",
    "# if flag_save_figs:\n",
    "#     fig.savefig(os.path.join(save_dir, 'trial_avg_pop_responses.pdf'), format='pdf')\n",
    "#     fig.savefig(os.path.join(save_dir, 'trial_avg_pop_responses.png'), format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do PCA to reduce dimensionality in the time-domain\n",
    "\n",
    "PCA: A linear algebra-based method to optimize how a set of variables can explain the variability of a dataset. Optimizing: meaning finding a new set of axes (ie. variables) that are linear combinations of the original axes where each new axis attempts to capture the most amount of variability in the data as possible while remaining linearly independent from the other new axes.\n",
    "\n",
    "In this case, we are finding a new linearly independent parameter space that maximizes the explained variance into the top new axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_pc_explained_var(explained_var, explained_var_thresh=90):\n",
    "    \"\"\"\n",
    "    Select pcs for those that capture more than threshold amount of variability in the data\n",
    "    \"\"\"\n",
    "    cum_sum = 0\n",
    "    for idx, PC_var in enumerate(explained_var):\n",
    "        cum_sum += PC_var\n",
    "        if cum_sum > explained_var_thresh:\n",
    "            return idx+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_savedpca_or_dopca = 'dopca'\n",
    "# Select 'dopca' for doing PCA on the data. Select 'savedpca' for loading my previous results\n",
    "\n",
    "# perform PCA across time\n",
    "if load_savedpca_or_dopca == 'dopca':\n",
    "    pca = PCA(n_components=min(populationdata.shape[0],populationdata.shape[1]), whiten=True)\n",
    "    pca.fit(populationdata) \n",
    "    with open(os.path.join(fdir, 'pcaresults.pickle'), 'wb') as f:\n",
    "        pickle.dump(pca, f)\n",
    "elif load_savedpca_or_dopca == 'savedpca':\n",
    "    with open(os.path.join(fdir, 'OFCCaMKII_pcaresults.pickle'), 'rb') as f:\n",
    "        pca = pickle.load(f)\n",
    "\n",
    "# pca across time\n",
    "transformed_data = pca.transform(populationdata)\n",
    "# transformed data: each ROI is now a linear combination of the original time-serie\n",
    "# np.save(os.path.join(save_dir, \"transformed_data.npy\"),transformed_data)\n",
    "\n",
    "# grab eigenvectors (pca.components_); linear combination of original axes\n",
    "pca_vectors = pca.components_ \n",
    "print(f'Number of PCs = {pca_vectors.shape[0]}')\n",
    "\n",
    "# Number of PCs to be kept is defined as the number at which the \n",
    "# scree plot bends. This is done by simply bending the scree plot\n",
    "# around the line joining (1, variance explained by first PC) and\n",
    "# (num of PCs, variance explained by the last PC) and finding the \n",
    "# number of components just below the minimum of this rotated plot\n",
    "x = 100*pca.explained_variance_ratio_ # eigenvalue ratios\n",
    "xprime = x - (x[0] + (x[-1]-x[0])/(x.size-1)*np.arange(x.size))\n",
    "\n",
    "# define number of PCs\n",
    "num_retained_pcs_scree = np.argmin(xprime)\n",
    "num_retained_pcs_var = num_pc_explained_var(x, 90)\n",
    "if pca_num_pc_method == 0:\n",
    "    num_retained_pcs = num_retained_pcs_scree\n",
    "elif pca_num_pc_method == 1:\n",
    "    num_retained_pcs = num_retained_pcs_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'Number of PCs to keep = {num_retained_pcs}')\n",
    "\n",
    "# plot PCA plot\n",
    "fig, ax = plt.subplots(figsize=(2,2))\n",
    "ax.plot(np.arange(pca.explained_variance_ratio_.shape[0]).astype(int)+1, x, 'k')\n",
    "ax.set_ylabel('Percentage of\\nvariance explained')\n",
    "ax.set_xlabel('PC number')\n",
    "ax.axvline(num_retained_pcs, linestyle='--', color='k', linewidth=0.5)\n",
    "ax.set_title('Scree plot')\n",
    "[i.set_linewidth(0.5) for i in ax.spines.values()]\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "fig.subplots_adjust(left=0.3)\n",
    "fig.subplots_adjust(right=0.98)\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "fig.subplots_adjust(top=0.9)\n",
    "\n",
    "# if flag_save_figs:\n",
    "#     fig.savefig(os.path.join(save_dir, 'scree_plot.png'), format='png', dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "### plot retained principal components\n",
    "numcols = 2.0\n",
    "fig, axs = plt.subplots(int(np.ceil(num_retained_pcs/numcols)), int(numcols), sharey='all',\n",
    "                        figsize=(2.2*numcols, 2.2*int(np.ceil(num_retained_pcs/numcols))))\n",
    "for pc in range(num_retained_pcs):\n",
    "    ax = axs.flat[pc]\n",
    "    for k, tempkey in enumerate(conditions):\n",
    "        ax.plot(tvec, pca_vectors[pc, k*window_size:(k+1)*window_size],\n",
    "                label='PC %d: %s'%(pc+1, tempkey))\n",
    "\n",
    "    ax.axvline(0, linestyle='--', color='k', linewidth=1)\n",
    "    ax.set_title(f'PC {pc+1}')\n",
    "\n",
    "    # labels\n",
    "    if pc == 0:\n",
    "        ax.set_xlabel('Time from cue (s)')\n",
    "        ax.set_ylabel( 'PCA weights')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "for ax in axs.flat[num_retained_pcs:]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "    \n",
    "# if flag_save_figs:\n",
    "#     fig.savefig(os.path.join(save_dir, 'PCA.png'), format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate optimal number of clusters and nearest neighbors using silhouette scores\n",
    "min_clusters = np.min([max_n_clusters+1, int(populationdata.shape[0])])\n",
    "possible_n_clusters = np.arange(2, max_n_clusters+1) #This requires a minimum of 2 clusters.\n",
    "# When the data contain no clusters at all, it will be quite visible when inspecting the two obtained clusters, \n",
    "# as the responses of the clusters will be quite similar. This will also be visible when plotting the data in\n",
    "# the reduced dimensionality PC space (done below).\n",
    "\n",
    "possible_clustering_models = np.array([\"Spectral\", \"Kmeans\"])\n",
    "silhouette_scores = np.nan*np.ones((possible_n_clusters.size,\n",
    "                                    possible_n_nearest_neighbors.size,\n",
    "                                    possible_clustering_models.size))\n",
    "\n",
    "# loop through iterations of clustering params\n",
    "for n_clustersidx, n_clusters in enumerate(possible_n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0) #tol=toler_options\n",
    "    for nnidx, nn in enumerate(possible_n_nearest_neighbors):\n",
    "        spectral = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', n_neighbors=nn, random_state=0)\n",
    "        models = [spectral,kmeans]\n",
    "        for modelidx,model in enumerate(models):\n",
    "            model.fit(transformed_data[:,:num_retained_pcs])\n",
    "            silhouette_scores[n_clustersidx, nnidx, modelidx] = silhouette_score(transformed_data[:,:num_retained_pcs],\n",
    "                                                                    model.labels_,\n",
    "                                                                    metric='cosine')\n",
    "            if modelidx == 0:\n",
    "                print(f'Done with numclusters = {n_clusters}, num nearest neighbors = {nn}: score = {silhouette_scores[n_clustersidx, nnidx, modelidx]}.3f')\n",
    "            else:\n",
    "                print(f'Done with numclusters = {n_clusters}, score = {silhouette_scores[n_clustersidx, nnidx, modelidx]}.3f')\n",
    "print(silhouette_scores.shape)\n",
    "print('Done with model fitting')\n",
    "\n",
    "silhouette_dict = {}\n",
    "silhouette_dict['possible_clustering_models'] = possible_clustering_models\n",
    "silhouette_dict['num_retained_pcs'] = num_retained_pcs\n",
    "silhouette_dict['possible_n_clusters'] = possible_n_clusters\n",
    "silhouette_dict['possible_n_nearest_neighbors'] = possible_n_nearest_neighbors\n",
    "silhouette_dict['silhouette_scores'] = silhouette_scores\n",
    "silhouette_dict['shape'] = 'cluster_nn'\n",
    "#with open(os.path.join(save_dir,'silhouette_scores.pkl'), 'wb') as f:\n",
    "#    pickle.dump(temp, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recluster with optimal params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify optimal parameters from the above parameter space\n",
    "temp = np.where(silhouette_dict['silhouette_scores']==np.nanmax(silhouette_dict['silhouette_scores']))\n",
    "\n",
    "n_clusters = silhouette_dict['possible_n_clusters'][temp[0][0]]\n",
    "n_nearest_neighbors = silhouette_dict['possible_n_nearest_neighbors'][temp[1][0]]\n",
    "num_retained_pcs = silhouette_dict['num_retained_pcs']\n",
    "method = silhouette_dict['possible_clustering_models'][temp[2][0]]\n",
    "print(f\"clusters: {n_clusters}, nearest neighbors: {n_nearest_neighbors}, PCs: {num_retained_pcs}, method: {method}\")\n",
    "\n",
    "# Redo clustering with these optimal parameters\n",
    "model = None\n",
    "if method == 'Spectral':\n",
    "    model = SpectralClustering(n_clusters=n_clusters,\n",
    "                           affinity='nearest_neighbors',\n",
    "                           n_neighbors=n_nearest_neighbors,\n",
    "                           random_state=0)\n",
    "else:\n",
    "    model = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "\n",
    "\n",
    "# model = AgglomerativeClustering(n_clusters=9,\n",
    "#                                 affinity='l1',\n",
    "#                                 linkage='average')\n",
    "\n",
    "model.fit(transformed_data[:,:num_retained_pcs])\n",
    "\n",
    "temp = silhouette_score(transformed_data[:,:num_retained_pcs], model.labels_, metric='cosine')\n",
    "\n",
    "print(f'Average silhouette score = {temp}.3f')\n",
    "\n",
    "# Save this optimal clustering model.\n",
    "# with open(os.path.join(save_dir, 'clusteringmodel.pickle'), 'wb') as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the clustering labels are arbitrary, I rename the clusters so that the first cluster will have the most\n",
    "# positive response and the last cluster will have the most negative response.\n",
    "def reorder_clusters(data, sort_win_frames, rawlabels):\n",
    "    uniquelabels = list(set(rawlabels))\n",
    "    responses = np.nan*np.ones((len(uniquelabels),))\n",
    "    for l, label in enumerate(uniquelabels):\n",
    "        responses[l] = np.mean(data[rawlabels==label, sort_win_frames[0]:sort_win_frames[1]])\n",
    "    temp = np.argsort(responses).astype(int)[::-1]\n",
    "    temp = np.array([np.where(temp==a)[0][0] for a in uniquelabels])\n",
    "    outputlabels = np.array([temp[a] for a in list(np.digitize(rawlabels, uniquelabels)-1)])\n",
    "    return outputlabels\n",
    "newlabels = reorder_clusters(populationdata, sortwindow_frames, model.labels_)\n",
    "\n",
    "# Create a new variable containing all unique cluster labels\n",
    "uniquelabels = list(set(newlabels))\n",
    "\n",
    "# np.save(os.path.join(summarydictdir, dt_string+'_'+ clusterkey+'_' + 'spectral_clusterlabels.npy'), newlabels)\n",
    "\n",
    "colors_for_cluster = plt.cm.viridis(np.linspace(0,1,len(uniquelabels)+3))\n",
    "\n",
    "np.save(os.path.join(save_dir, 'cluster_labels.npy'), newlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot z-score activity for each cluster over time\n",
    "sortwindow = [15, 100]\n",
    "\n",
    "fig, axs = plt.subplots(len(conditions),len(uniquelabels),\n",
    "                        figsize=(2*len(uniquelabels),2*len(conditions)))\n",
    "if len(axs.shape) == 1:\n",
    "    axs = np.expand_dims(axs, axis=0)\n",
    "\n",
    "numroisincluster = np.nan*np.ones((len(uniquelabels),))\n",
    "\n",
    "for c, cluster in enumerate(uniquelabels):\n",
    "    for k, tempkey in enumerate(conditions):\n",
    "        temp = populationdata[np.where(newlabels==cluster)[0], k*window_size:(k+1)*window_size]\n",
    "        numroisincluster[c] = temp.shape[0]\n",
    "        ax=axs[k, cluster]\n",
    "        sortresponse = np.argsort(np.mean(temp[:,sortwindow[0]:sortwindow[1]], axis=1))[::-1]\n",
    "        \n",
    "        plot_extent = [tvec[0], tvec[-1], len(sortresponse), 0 ]\n",
    "        im = utils.subplot_heatmap(ax, ' ', temp[sortresponse], \n",
    "                                   clims = [-cmax*heatmap_cmap_scaling, cmax*heatmap_cmap_scaling], extent_=plot_extent)\n",
    "\n",
    "        axs[k, cluster].grid(False) \n",
    "        if k!=len(conditions)-1:\n",
    "\n",
    "            axs[k, cluster].set_xticks([])\n",
    "\n",
    "        axs[k, cluster].set_yticks([])\n",
    "        axs[k, cluster].axvline(0, linestyle='--', color='k', linewidth=0.5)\n",
    "        if flag_plot_reward_line:\n",
    "            axs[k, cluster].axvline(second_event_seconds, linestyle='--', color='k', linewidth=0.5)\n",
    "        if cluster==0:\n",
    "            axs[k, 0].set_ylabel('%s'%(tempkey))\n",
    "    axs[0, cluster].set_title('Cluster %d\\n(n=%d)'%(cluster+1, numroisincluster[c]))\n",
    "    \n",
    "fig.text(0.5, 0.05, 'Time from cue (s)', fontsize=12,\n",
    "         horizontalalignment='center', verticalalignment='center', rotation='horizontal')\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "fig.subplots_adjust(left=0.03)\n",
    "fig.subplots_adjust(right=0.93)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "fig.subplots_adjust(top=0.83)                                    \n",
    "\n",
    "cbar = fig.colorbar(im, ax = axs, shrink = 0.7)\n",
    "cbar.ax.set_ylabel('Z-Score Activity', fontsize=13);\n",
    "\n",
    "if flag_save_figs:\n",
    "    plt.savefig(os.path.join(save_dir, 'cluster_heatmap.png'))\n",
    "    plt.savefig(os.path.join(save_dir, 'cluster_heatmap.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec_convert_dict = {}\n",
    "for i in range(len(tvec)):\n",
    "    tvec_convert_dict[i] = tvec[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot amount of fluorescence normalized for each cluster by conditions over time\n",
    "fig, axs = plt.subplots(1,len(uniquelabels),\n",
    "                        figsize=(3*len(uniquelabels),1.5*len(conditions)))\n",
    "\n",
    "for c, cluster in enumerate(uniquelabels):\n",
    "\n",
    "    for k, tempkey in enumerate(conditions):\n",
    "        temp = populationdata[np.where(newlabels==cluster)[0], k*window_size:(k+1)*window_size]\n",
    "        numroisincluster[c] = temp.shape[0]\n",
    "        sortresponse = np.argsort(np.mean(temp[:,sortwindow[0]:sortwindow[1]], axis=1))[::-1]\n",
    "        sns.lineplot(x=\"variable\", y=\"value\",data = pd.DataFrame(temp[sortresponse]).rename(columns=tvec_convert_dict).melt(),\n",
    "                    ax = axs[cluster],\n",
    "                    palette=plt.get_cmap('coolwarm'),label = tempkey,legend = False)\n",
    "        axs[cluster].grid(False)  \n",
    "        axs[cluster].axvline(0, linestyle='--', color='k', linewidth=0.5)\n",
    "        axs[cluster].spines['right'].set_visible(False)\n",
    "        axs[cluster].spines['top'].set_visible(False)\n",
    "        if cluster==0:\n",
    "            axs[cluster].set_ylabel('Normalized fluorescence')\n",
    "        else:\n",
    "            axs[cluster].set_ylabel('')\n",
    "        axs[cluster].set_xlabel('')\n",
    "    axs[cluster].set_title('Cluster %d\\n(n=%d)'%(cluster+1, numroisincluster[c]))\n",
    "    axs[0].legend()\n",
    "fig.text(0.5, 0.05, 'Time from cue (s)', fontsize=12,\n",
    "         horizontalalignment='center', verticalalignment='center', rotation='horizontal')\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "fig.subplots_adjust(left=0.03)\n",
    "fig.subplots_adjust(right=0.93)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "fig.subplots_adjust(top=0.83)\n",
    "\n",
    "if flag_save_figs:\n",
    "    plt.savefig(os.path.join(save_dir, 'cluster_roiAvg_traces.png'))\n",
    "    plt.savefig(os.path.join(save_dir, 'cluster_roiAvg_traces.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform TSNE on newly defined clusters\n",
    "num_clusterpairs = len(uniquelabels)*(len(uniquelabels)-1)/2\n",
    "\n",
    "numrows = int(np.ceil(num_clusterpairs**0.5))\n",
    "numcols = int(np.ceil(num_clusterpairs/np.ceil(num_clusterpairs**0.5)))\n",
    "fig, axs = plt.subplots(numrows, numcols, figsize=(3*numrows, 3*numcols))\n",
    "\n",
    "tempsum = 0\n",
    "for c1, cluster1 in enumerate(uniquelabels):\n",
    "    for c2, cluster2 in enumerate(uniquelabels):\n",
    "        if cluster1>=cluster2:\n",
    "            continue\n",
    "\n",
    "        temp1 = transformed_data[np.where(newlabels==cluster1)[0], :num_retained_pcs]\n",
    "        temp2 = transformed_data[np.where(newlabels==cluster2)[0], :num_retained_pcs]\n",
    "        X = np.concatenate((temp1, temp2), axis=0)\n",
    "\n",
    "        tsne = TSNE(n_components=2, init='random',\n",
    "                    random_state=0, perplexity=np.sqrt(X.shape[0]))\n",
    "        Y = tsne.fit_transform(X)\n",
    "\n",
    "        if numrows*numcols==1:\n",
    "            ax = axs\n",
    "        else:\n",
    "            ax = axs[int(tempsum/numcols),\n",
    "                     abs(tempsum - int(tempsum/numcols)*numcols)]\n",
    "        ax.scatter(Y[:np.sum(newlabels==cluster1),0],\n",
    "                   Y[:np.sum(newlabels==cluster1),1],\n",
    "                   color=colors_for_cluster[cluster1], label='Cluster %d'%(cluster1+1), alpha=1)\n",
    "        ax.scatter(Y[np.sum(newlabels==cluster1):,0],\n",
    "                   Y[np.sum(newlabels==cluster1):,1],\n",
    "                   color=colors_for_cluster[cluster2+3], label='Cluster %d'%(cluster2+1), alpha=1)\n",
    "\n",
    "        ax.set_xlabel('tsne dimension 1')\n",
    "        ax.set_ylabel('tsne dimension 2')\n",
    "        ax.legend()\n",
    "        tempsum += 1\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
