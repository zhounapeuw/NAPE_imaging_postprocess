{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3bfeae-d964-4275-a152-0a2a3a3a1ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7427dde-16ab-407d-9591-cd6b746aec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "USER-DEFINED VARIABLES\n",
    "\"\"\"\n",
    "\n",
    "def define_params(method = 'single'):\n",
    "    \n",
    "    fparams = {}\n",
    "    \n",
    "    if method == 'single':\n",
    "        \n",
    "        # fdir signifies to the root path of the data. Currently, the abspath phrase points to sample data from the repo.\n",
    "        # To specify a path that is on your local computer, use this string format: r'your_root_path', where you should copy/paste\n",
    "        # your path between the single quotes (important to keep the r to render as a complete raw string). See example below:\n",
    "        # r'C:\\Users\\stuberadmin\\Documents\\GitHub\\NAPE_imaging_postprocess\\napeca_post\\sample_data' \n",
    "        fparams['fdir_parent'] = r'D:\\olympus_data\\NAPECA_TEST_DATA'\n",
    "        fparams['fname'] = os.path.split(fparams['fdir_parent'])[1]\n",
    "        \n",
    "        fparams['flag_close_figs_after_save'] = True\n",
    "        fparams['figs_save_dir'] = r'D:\\olympus_data\\NAPECA_TEST_DATA'\n",
    "        fparams['flag_save_figs'] = True\n",
    "        \n",
    "        # set the sampling rate\n",
    "        fparams['fs'] = 5 # this gets overwritten by json fs variable (if it exists) that is saved in preprocessing\n",
    "\n",
    "        # set to None if want to include all conditions from behav data; \n",
    "        # otherwise, set to list of conditions, eg. ['plus', 'minus']\n",
    "        fparams['selected_conditions'] = None \n",
    "        \n",
    "        # trial windowing and normalization\n",
    "        fparams['trial_start_end'] = [-2, 8] # primary visualization window relative to event onset; [start, end] times (in seconds) \n",
    "        fparams['flag_normalization'] = 'zscore' # options: 'zscore', 'dff', None\n",
    "        fparams['specific_baseline'] = False\n",
    "        fparams['baseline_start_end'] = [-2, -0.2] # baseline window (in seconds) for performing baseline normalization. either a list [start, end] or an int/float (see details in markdown above); I set this to -0.2 to be safe I'm not grabbing a sample that includes the event\n",
    "        fparams['event_dur'] = 2 # duration of stim/event in seconds; displays a line below main plot indicating event duration\n",
    "        fparams['event_sort_analysis_win'] = [0, 5] # time window (in seconds) for sorting cells; list [start, end]\n",
    "        \n",
    "        # ROI sorting; if flag_sort_rois is set to True, ROIs are sorted by activity in the fparams['event_sort_analysis_win'] window\n",
    "        fparams['flag_sort_rois'] = False\n",
    "        if fparams['flag_sort_rois']:\n",
    "            \n",
    "            fparams['user_sort_method'] = 'max_value' # peak_time or max_value\n",
    "            fparams['roi_sort_cond'] = 'plus' # for roi-resolved heatmaps, which condition to sort ROIs by\n",
    "\n",
    "        fparams['interesting_rois'] = []\n",
    "\n",
    "        # errorbar and saving figures\n",
    "        fparams['flag_roi_trial_avg_errbar'] = True # toggle to show error bar on roi- and trial-averaged traces\n",
    "        fparams['flag_trial_avg_errbar'] = True # toggle to show error bars on the trial-avg traces\n",
    "        ########## DONE WITH USER-DEFINED VARIABLES; DON'T MODIFY CODE BELOW\n",
    "        \n",
    "        if fparams['flag_normalization'].lower() == 'zscore':\n",
    "            fparams['norm_mode'] = 'zdata'\n",
    "        elif fparams['flag_normalization'].lower() == 'dff':\n",
    "            fparams['norm_mode'] = 'dff_data'\n",
    "        else:\n",
    "            fparams['norm_mode'] = 'data'\n",
    "    \n",
    "    return fparams\n",
    "\n",
    "fparams = define_params(method = 'single') # options are 'single', 'f2a', 'root_dir'\n",
    "runtime_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181bcf8c-109b-4790-b839-1a89939a17f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple class to update limits as you go through iterations of data\n",
    "# first call update_lims(first_lims)\n",
    "# then update_lims.update(new_lims)\n",
    "# update_lims.output() outputs lims\n",
    "class update_lims:\n",
    "    \n",
    "    def __init__(self, lims):\n",
    "        self.lims = lims\n",
    "        \n",
    "    \n",
    "    def update(self, new_lims):\n",
    "        if self.lims[0] > new_lims[0]:\n",
    "            self.lims[0] = new_lims[0]\n",
    "        \n",
    "        if self.lims[1] < new_lims[1]:\n",
    "            self.lims[1] = new_lims[1]\n",
    "\n",
    "    def output(self):\n",
    "        return self.lims\n",
    "    \n",
    "    \n",
    "# find 2D subplot index based on a numerical incremented index (ie. idx=3 would be (2,1) for a 2x2 subplot figure)     \n",
    "def subplot_loc(idx, num_rows, num_col):\n",
    "    if n_rows == 1:\n",
    "        subplot_index = idx\n",
    "    else:\n",
    "        subplot_index = np.unravel_index(idx, (n_rows, int(n_columns))) # turn int index to a tuple of array coordinates\n",
    "    return subplot_index\n",
    "\n",
    "\n",
    "def get_cmap(n, name='plasma'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "\n",
    "\n",
    "def is_all_nans(vector):\n",
    "    \"\"\"\n",
    "    checks if series or vector contains all nans; returns boolean. Used to identify and exclude all-nan rois\n",
    "    \"\"\"\n",
    "    if isinstance(vector, pd.Series):\n",
    "        vector = vector.values\n",
    "    return np.isnan(vector).all()\n",
    "\n",
    "# declare some fixed constant variables\n",
    "axis_label_size = 15\n",
    "tick_font_size = 14 \n",
    "\n",
    "if 'zscore' in fparams['flag_normalization']:\n",
    "        data_trial_resolved_key = 'zdata'\n",
    "        data_trial_avg_key = 'ztrial_avg_data'\n",
    "        cmap_ = None\n",
    "        ylabel = 'Z-score Activity'\n",
    "else:\n",
    "    data_trial_resolved_key = 'data'\n",
    "    data_trial_avg_key = 'trial_avg_data'\n",
    "    cmap_ = 'inferno'\n",
    "    ylabel = 'Activity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17aac29-08a8-4c6d-b16b-085d4e14d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_events(fparams_in, fpath):\n",
    "    \n",
    "    glob_event_files = glob.glob(fpath) # look for a file in specified directory\n",
    "    if len(glob_event_files) == 0:\n",
    "        print(f'{events_file_path} not detected. Please check if path is correct.')\n",
    "    if 'csv' in glob_event_files[0]:\n",
    "        event_times = utils.df_to_dict(glob_event_files[0])\n",
    "    elif any(x in glob_event_files[0] for x in ['pkl', 'pickle']):\n",
    "        event_times = pickle.load( open( glob_event_files[0], \"rb\" ), fix_imports=True, encoding='latin1' ) # latin1 b/c original pickle made in python 2\n",
    "    event_frames = utils.dict_time_to_samples(event_times, fparams_in['fs'])\n",
    "\n",
    "    return event_times, event_frames\n",
    "\n",
    "def get_conditions(fparams_in, event_dict):\n",
    "    \n",
    "    # identify conditions to analyze\n",
    "    all_conditions = event_dict.keys()\n",
    "    conditions = [ condition for condition in all_conditions if len(event_dict[condition]) > 0 ] # keep conditions that have events\n",
    "    \n",
    "    conditions.sort()\n",
    "    if fparams_in['selected_conditions']:\n",
    "        conditions = fparams_in['selected_conditions']\n",
    "\n",
    "    return conditions\n",
    "\n",
    "def calc_event_time_vars(fparams_in):\n",
    "\n",
    "    dict = {}\n",
    "    \n",
    "    dict['trial_start_end_sec'] = np.array(fparams_in['trial_start_end']) # trial windowing in seconds relative to ttl-onset/trial-onset, in seconds\n",
    "    if type(fparams_in['baseline_start_end']) is list:\n",
    "        baseline_start_end_sec = np.array(fparams_in['baseline_start_end'])\n",
    "    elif isinstance(fparams_in['baseline_start_end'], (int, float)):\n",
    "        baseline_start_end_sec = np.array([dict['trial_start_end_sec'][0], fparams_in['baseline_start_end']])\n",
    "    \n",
    "    # convert times to samples and get sample vector for the trial \n",
    "    dict['trial_begEnd_samp'] = np.round(dict['trial_start_end_sec']*fparams_in['fs']).astype('int') # turn trial start/end times to samples\n",
    "    # and for baseline period\n",
    "    dict['baseline_begEnd_samp'] = np.round(baseline_start_end_sec*fparams_in['fs']).astype('int')\n",
    "    \n",
    "    # calculate time vector for plot x axes\n",
    "    trial_svec = np.arange(dict['trial_begEnd_samp'][0], dict['trial_begEnd_samp'][1])\n",
    "    num_samples_trial = len( trial_svec )\n",
    "    dict['tvec'] = np.round(np.linspace(dict['trial_start_end_sec'][0], dict['trial_start_end_sec'][1], num_samples_trial+1), 2)\n",
    "    \n",
    "    # find samples and calculations for time 0 for plotting\n",
    "    dict['t0_sample'] = utils.get_tvec_sample(dict['tvec'], 0) # grabs the sample index of a given time from a vector of times\n",
    "    dict['event_end_sample'] = int(np.round(dict['t0_sample']+fparams_in['event_dur']*fparams_in['fs']))\n",
    "    dict['event_bound_ratio'] = [(dict['t0_sample'])/num_samples_trial , dict['event_end_sample']/num_samples_trial] # fraction of total samples for event start and end; only used for plotting line indicating event duration\n",
    "\n",
    "    return dict\n",
    "\n",
    "def run_single_session(fparams_in, runtime_params):\n",
    "\n",
    "    ### declare paths \n",
    "    signals_fpath = os.path.join(fparams_in['fdir'], fparams_in['fname_signal'])\n",
    "    events_file_path = os.path.join(fparams_in['fdir'], fparams_in['fname_events'])\n",
    "    \n",
    "    save_dir = os.path.join(fparams_in['fdir'], 'event_rel_analysis')\n",
    "    \n",
    "    utils.check_exist_dir(save_dir); # make the save directory\n",
    "    \n",
    "    ### create variables that reference samples and times for slicing and plotting the data\n",
    "\n",
    "    timing_dict = calc_event_time_vars(fparams_in)\n",
    "    \n",
    "    ### load data\n",
    "    signals = utils.load_signals(signals_fpath)\n",
    "    \n",
    "    runtime_params['num_rois'] = signals.shape[0]\n",
    "    runtime_params['all_nan_rois'] = np.where(np.apply_along_axis(is_all_nans, 1, signals)) # find rois with activity as all nans\n",
    "    \n",
    "    ### load behavioral data and trial info\n",
    "\n",
    "    event_times, event_frames = load_events(fparams_in, events_file_path)\n",
    "    conditions = get_conditions(fparams_in, event_frames)\n",
    "\n",
    "    cmap_lines = get_cmap(len(conditions))\n",
    "    \n",
    "    \"\"\"\n",
    "    MAIN data processing function to extract event-centered data\n",
    "    \n",
    "    extract and save trial data, \n",
    "    saved data are in the event_rel_analysis subfolder, a pickle file that contains the extracted trial data\n",
    "    \"\"\"\n",
    "    data_dict = utils.extract_trial_data(signals, timing_dict['tvec'], timing_dict['trial_begEnd_samp'], event_frames, \n",
    "                                         conditions, fparams_in['specific_baseline'], baseline_start_end_samp = timing_dict['baseline_begEnd_samp'], save_dir=save_dir)\n",
    "\n",
    "    return runtime_params, conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd500a5f-16f9-40a5-940c-68b182a31d68",
   "metadata": {},
   "source": [
    "## Run single session event-related analysis on all subfolders/sessions in parent directory (fdir_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c701e-44fa-4571-8ae4-d5bdc8770531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_files(parent_directory):\n",
    "    \"\"\"\n",
    "    Collect npy and csv files from subdirectories one level down in the given parent directory.\n",
    "    \n",
    "    Parameters:\n",
    "        parent_directory (str): Path to the parent directory.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are subdirectory names and values are \n",
    "              dictionaries with file paths to 'signals' npy and 'events' csv files.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    # List all immediate subdirectories of the parent directory\n",
    "    for subdir_name in os.listdir(parent_directory):\n",
    "        subdir_path = os.path.join(parent_directory, subdir_name)\n",
    "        \n",
    "        # Skip if it's not a directory\n",
    "        if not os.path.isdir(subdir_path):\n",
    "            continue\n",
    "        \n",
    "        signals_file = None\n",
    "        events_file = None\n",
    "\n",
    "        # Look for files in the current subdirectory\n",
    "        for file in os.listdir(subdir_path):\n",
    "            if file.endswith('.npy'):\n",
    "                signals_file = os.path.join(subdir_path, file)\n",
    "                \n",
    "                if 'signals' not in file:\n",
    "                    warnings.warn(f\"No 'signals' npy file found in {subdir_path}\")\n",
    "            elif file.endswith('.csv'):\n",
    "                events_file = os.path.join(subdir_path, file)\n",
    "                \n",
    "                if 'events' not in file:\n",
    "                    warnings.warn(f\"No 'events' csv file found in {subdir_path}\")\n",
    "            \n",
    "        \n",
    "        # Store the results (including partial findings)\n",
    "        result[subdir_name] = {\n",
    "            'signals': signals_file,\n",
    "            'events': events_file\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def update_fs(fparams):\n",
    "    json_fpath = os.path.join(fparams['fdir_parent'], fparams['fname']+\".json\")\n",
    "    if os.path.exists(json_fpath):\n",
    "        json_data = utils.open_json(json_fpath)\n",
    "        if 'fs' in json_data:\n",
    "            fparams['fs'] = json_data['fs']\n",
    "            \n",
    "with open(os.path.join(fparams['fdir_parent'], 'group_event_rel_analysis.json'), 'w') as fp:\n",
    "    json.dump(fparams, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4ada0-b0df-4934-a7fa-dc97d0b87260",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_mapping = collect_files(fparams['fdir_parent'])\n",
    "\n",
    "# run automatic single-session analysis\n",
    "for subdir, files in file_mapping.items():\n",
    "    print(f\"{subdir}: {files}\")\n",
    "    \n",
    "    fparam = fparams.copy() # copy requires or original fparams will modified with subsequent actions\n",
    "    fparam['fdir'] = os.path.dirname(files['signals'])\n",
    "    fparam['fname_signal'] = os.path.basename(files['signals'])\n",
    "    fparam['fname_events'] = os.path.basename(files['events'])\n",
    "\n",
    "    fparam['fname'] = os.path.split(fparams['fdir_parent'])[1]\n",
    "    \n",
    "    runtime_params, conditions = run_single_session(fparam, runtime_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39abab65-f61c-4643-b542-39c6446ab593",
   "metadata": {},
   "source": [
    "## Perform group level analysis after single session analysis has completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02b764-e91c-47e4-aa56-680fc33b828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find all .pkl files containing \"event\" in their name in a directory and its subdirectories\n",
    "def find_pkl_files(root_dir):\n",
    "    \"\"\"Recursively finds all .pkl files containing 'event' in the given directory.\"\"\"\n",
    "    pkl_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if 'event_data_dict' in file and file.endswith('.pkl'):\n",
    "                pkl_files.append(os.path.join(dirpath, file))\n",
    "    return pkl_files\n",
    "\n",
    "# Function to load and process data from a .pkl file\n",
    "def load_and_average_data(fparams_in, file_path):\n",
    "    \"\"\"Loads a .pkl file and extracts trial-averaged data.\"\"\"\n",
    "    print(f\"Loading {file_path}\")\n",
    "    with open(file_path, 'rb') as file:\n",
    "        session_data = pickle.load(file)\n",
    "    \n",
    "    processed_data = {}\n",
    "    for condition, content in session_data.items():\n",
    "        if 'data' in content:\n",
    "            data_array = np.array(content[fparams_in['norm_mode']])  # Convert to NumPy array\n",
    "            trial_avg = np.mean(data_array, axis=0)  # Average across trials\n",
    "            processed_data[condition] = trial_avg\n",
    "    return processed_data\n",
    "\n",
    "# Main function to structure and combine trial-averaged data across sessions\n",
    "def combine_sessions(fparams_in):\n",
    "    \"\"\"Combines trial-averaged data across multiple sessions.\"\"\"\n",
    "    all_data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    pkl_files = find_pkl_files(fparams_in['fdir_parent'])\n",
    "    for file_idx, file_path in enumerate(pkl_files):\n",
    "        session_data = load_and_average_data(fparams_in, file_path) # session_data are trial-averaged \n",
    "        for condition, trial_avg in session_data.items():\n",
    "            all_data[condition][file_idx] = trial_avg\n",
    "    \n",
    "    # Organize data for flexibility\n",
    "    structured_data = {}\n",
    "    for condition, sessions in all_data.items():\n",
    "        structured_data[condition] = {\n",
    "            session_id: np.stack(trials, axis=0) if len(trials) > 1 else np.expand_dims(trials[0], axis=0)\n",
    "            for session_id, trials in sessions.items()\n",
    "        }  # Ensures compatibility with single-cell data by adding an axis if needed\n",
    "        \n",
    "    return structured_data\n",
    "\n",
    "\n",
    "# calculate all the color limits for heatmaps; useful for locking color limits across different heatmap subplots   \n",
    "def generate_clims(data_in, norm_type):\n",
    "    # get min and max for all data across conditions \n",
    "    clims_out = [np.nanmin(data_in), np.nanmax(data_in)]\n",
    "    if 'zscore' in norm_type: # if data are zscored, make limits symmetrical and centered at 0\n",
    "        clims_max = np.max(np.abs(clims_out)) # then we take the higher of the two magnitudes\n",
    "        clims_out = [-clims_max*0.5, clims_max*0.5] # and set it as the negative and positive limit for plotting\n",
    "    return clims_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4a843-f776-43c8-8ebf-e593feb80013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    structured_data = combine_sessions(fparams)\n",
    "\n",
    "    # Example analyses:\n",
    "    # 1. Average across cells for each session, then collate cell-averaged traces into avg_across_cells\n",
    "    data_dict_cell_avg = {}\n",
    "    for condition, sessions in structured_data.items():\n",
    "        session_data = np.array([np.mean(data_array, axis=0) for data_array in sessions.values()])\n",
    "        data_dict_cell_avg[condition] = session_data\n",
    "        print(f\"Condition: {condition}, Avg Across Cells Shape (sessions, time): {data_dict_cell_avg[condition].shape}\")\n",
    "    \n",
    "    # 2. Combine cells across sessions for cumulative averaging\n",
    "    data_dict_cumulative_cells = {}\n",
    "    for condition, sessions in structured_data.items():\n",
    "        # Stack all session arrays along a new axis, then reshape into blocks\n",
    "        session_arrays = [np.array(data_array) for data_array in sessions.values()]\n",
    "        cumulative_data = np.vstack(session_arrays)  # Ensures session-wise grouping\n",
    "        data_dict_cumulative_cells[condition] = cumulative_data\n",
    "        print(f\"Condition: {condition}, Cumulative Cells Shape (total_cells, time): {cumulative_data.shape}\")\n",
    "\n",
    "    \n",
    "    data_dict = data_dict_cumulative_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f24514-6563-4e10-b726-3e4b9ab08946",
   "metadata": {},
   "source": [
    "## Process data combined across sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7fc149-da7f-411c-b593-ca364c482a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find closest sample when a time occurs in a time vector\n",
    "tvec2samp = lambda tvec, time: np.argmin(np.abs(tvec - time))\n",
    "\n",
    "# function to sort ROIs based on activity in certain epoch\n",
    "def sort_heatmap_peaks(data, tvec, sort_epoch_start_time, sort_epoch_end_time, sort_method = 'peak_time'):\n",
    "    \n",
    "    # find start/end samples for epoch\n",
    "    sort_epoch_start_samp = tvec2samp(tvec, sort_epoch_start_time)\n",
    "    sort_epoch_end_samp = tvec2samp(tvec, sort_epoch_end_time)\n",
    "    \n",
    "    if sort_method == 'peak_time':\n",
    "        epoch_peak_samp = np.argmax(data[:,sort_epoch_start_samp:sort_epoch_end_samp], axis=1)\n",
    "        final_sorting = np.argsort(epoch_peak_samp)\n",
    "        \n",
    "    elif sort_method == 'max_value':\n",
    "        time_max = np.nanmax(data[:,sort_epoch_start_samp:sort_epoch_end_samp], axis=1)\n",
    "        final_sorting = np.argsort(time_max)[::-1]\n",
    "        \n",
    "    elif sort_method == 'mean_value':\n",
    "        epoch_peak_samp = np.mean(data[:,sort_epoch_start_samp:sort_epoch_end_samp], axis=1)\n",
    "        final_sorting = np.flip(np.argsort(epoch_peak_samp))\n",
    "\n",
    "    return final_sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b11f87-3564-40bc-a4cb-17a9b3706e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_dict = calc_event_time_vars(fparams)\n",
    "\n",
    "# if flag is true, sort ROIs (usually by average fluorescence within analysis window)\n",
    "if fparams['flag_sort_rois']:\n",
    "    if not fparams['roi_sort_cond']: # if no condition to sort by specified, use first condition\n",
    "        fparams['roi_sort_cond'] = data_dict.keys()[0]\n",
    "    if not fparams['roi_sort_cond'] in data_dict.keys():\n",
    "        sorted_roi_order = range(runtime_params['num_rois'])\n",
    "        interesting_rois = fparams['interesting_rois']\n",
    "        print('Specified condition to sort by doesn\\'t exist! ROIs are in default sorting.')\n",
    "    else:\n",
    "        # returns new order of rois sorted using the data and method supplied in the specified window\n",
    "        sorted_roi_order = sort_heatmap_peaks(data_dict[fparams['roi_sort_cond']], timing_dict['tvec'], \n",
    "                           sort_epoch_start_time=0, \n",
    "                           sort_epoch_end_time = timing_dict['trial_start_end_sec'][-1], \n",
    "                           sort_method = fparams['user_sort_method'])\n",
    "        # finds corresponding interesting roi (roi's to mark with an arrow) order after sorting\n",
    "        interesting_rois = np.in1d(sorted_roi_order, fparams['interesting_rois']).nonzero()[0] \n",
    "else:\n",
    "    sorted_roi_order = None\n",
    "    interesting_rois = fparams['interesting_rois']\n",
    "\n",
    "if not runtime_params['all_nan_rois'][0].size == 0:\n",
    "    set_diff_keep_order = lambda main_list, remove_list : [i for i in main_list if i not in remove_list]\n",
    "    sorted_roi_order = set_diff_keep_order(sorted_roi_order, runtime_params['all_nan_rois'])\n",
    "    interesting_rois = [i for i in fparams['interesting_rois'] if i not in runtime_params['all_nan_rois']]\n",
    "    \n",
    "roi_order_path = os.path.join(fparams['fdir_parent'], fparams['fname'] + '_roi_order.pkl')\n",
    "with open(roi_order_path, 'wb') as handle:\n",
    "     pickle.dump(sorted_roi_order, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ec6ef-b6fa-47fa-8f14-3f27140c8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trial_avg_heatmap(data_in, conditions, tvec, event_bound_ratio, cmap, clims, sorted_roi_order = None, \n",
    "                           rois_oi = None, save_dir = None, axis_label_size=15):\n",
    "    \n",
    "    \"\"\"\n",
    "    Technically doesn't need to remove all_nan_rois b/c of nanmean calculations\n",
    "    \"\"\"\n",
    "    \n",
    "    num_subplots = len(conditions)\n",
    "    n_columns = np.min([num_subplots, 3.0])\n",
    "    n_rows = int(np.ceil(num_subplots/n_columns))\n",
    "\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(nrows=n_rows, ncols=int(n_columns), figsize = (n_columns*5, n_rows*4))\n",
    "    if not isinstance(ax,np.ndarray): # this is here to make the code below compatible with indexing a single subplot object\n",
    "        ax = [ax]\n",
    "\n",
    "    for idx, cond in enumerate(conditions):\n",
    "\n",
    "        num_rois = data_in[cond].shape[0]\n",
    "\n",
    "        # set imshow extent to replace x and y axis ticks/labels (replace samples with time)\n",
    "        plot_extent = [tvec[0], tvec[-1], num_rois, 0 ]\n",
    "        \n",
    "        # determine subplot location index\n",
    "        if n_rows == 1:\n",
    "            subplot_index = idx\n",
    "        else:\n",
    "            subplot_index = np.unravel_index(idx, (n_rows, int(n_columns))) # turn int index to a tuple of array coordinates\n",
    "\n",
    "        # prep labels; plot x and y labels for first subplot\n",
    "        if subplot_index == (0, 0) or subplot_index == 0 :\n",
    "            ax[subplot_index].set_ylabel('ROI #', fontsize=axis_label_size)\n",
    "            ax[subplot_index].set_xlabel('Time (s)', fontsize=axis_label_size);\n",
    "        ax[subplot_index].tick_params(axis = 'both', which = 'major', labelsize = tick_font_size)\n",
    "        \n",
    "        # plot the data\n",
    "        if sorted_roi_order is not None:\n",
    "            roi_order = sorted_roi_order\n",
    "        else:\n",
    "            roi_order = slice(0, num_rois)\n",
    "        to_plot = data_in[cond][roi_order,:] # \n",
    "        \n",
    "        im = utils.subplot_heatmap(ax[subplot_index], cond, to_plot, cmap=cmap_, clims=clims, extent_=plot_extent)\n",
    "        ax[subplot_index].axvline(0, color='k', alpha=0.3) # plot vertical line for time zero\n",
    "        ax[subplot_index].annotate('', xy=(event_bound_ratio[0], -0.01), xycoords='axes fraction', \n",
    "                                       xytext=(event_bound_ratio[1], -0.01), \n",
    "                                       arrowprops=dict(arrowstyle=\"-\", color='g'))\n",
    "        if rois_oi is not None:\n",
    "            for ROI_OI in rois_oi:\n",
    "                ax[subplot_index].annotate('', xy=(1.005, 1-(ROI_OI/num_rois)-0.015), xycoords='axes fraction', \n",
    "                                           xytext=(1.06, 1-(ROI_OI/num_rois)-0.015), \n",
    "                                           arrowprops=dict(arrowstyle=\"->\", color='k'))\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    cbar = fig.colorbar(im, ax = ax, shrink = 0.7)\n",
    "    cbar.ax.set_ylabel(ylabel, fontsize=13)\n",
    "    \n",
    "    # hide empty subplot\n",
    "    if num_subplots > 1:\n",
    "        for a in ax.flat[num_subplots:]:\n",
    "            a.axis('off')\n",
    "    \n",
    "    if save_dir:\n",
    "        fig.savefig(os.path.join(save_dir,'trial_avg_heatmap.png')); \n",
    "        fig.savefig(os.path.join(save_dir,'trial_avg_heatmap.pdf'));\n",
    "\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f052b64-49ea-4a8f-b895-58bcd7543ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# major change - input into plot_trial_avg_heatmap data_dict does not have a 2nd key tier of normalization type - that will be determined earlier\n",
    "# it's just data_dict[condition]\n",
    "\n",
    "fig, ax = plot_trial_avg_heatmap(data_dict, conditions, timing_dict['tvec'], timing_dict['event_bound_ratio'], cmap_,\n",
    "                       clims = generate_clims(np.concatenate([data_dict[cond].flatten() for cond in conditions]), \n",
    "                                              fparams['flag_normalization']),\n",
    "                       sorted_roi_order = sorted_roi_order, rois_oi = interesting_rois, save_dir = fparams['figs_save_dir'])\n",
    "\n",
    "#fig.set_size_inches(12, 15)\n",
    "#ax[0].set_ylim(0,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13fbc2-47c7-4aff-ad87-6ea42aab0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_shades = []\n",
    "cmap_lines = get_cmap(len(conditions))\n",
    "\n",
    "fig, axs = plt.subplots(1,1, figsize = (10,6))\n",
    "for idx, cond in enumerate(conditions):\n",
    "    line_color = cmap_lines(idx)\n",
    "\n",
    "    num_rois = data_dict[cond].shape[0]\n",
    "    \n",
    "    # take avg/std across ROIs; data are already trial-avged\n",
    "    roi_trial_avg = np.nanmean(data_dict[cond], axis=0)\n",
    "    roi_trial_std = np.nanstd(data_dict[cond], axis=0)\n",
    "     \n",
    "    to_plot = np.squeeze(roi_trial_avg)\n",
    "    to_plot_err = np.squeeze(roi_trial_std)/np.sqrt(num_rois)\n",
    "    \n",
    "    axs.plot(timing_dict['tvec'], to_plot, color=line_color)\n",
    "\n",
    "    \n",
    "    \n",
    "    if fparams['flag_roi_trial_avg_errbar']:\n",
    "        line = axs.plot(timing_dict['tvec'][timing_dict['t0_sample']:timing_dict['event_end_sample']], to_plot[timing_dict['t0_sample']:timing_dict['event_end_sample']], color=line_color)\n",
    "        shade = axs.fill_between(timing_dict['tvec'], to_plot - to_plot_err, to_plot + to_plot_err, color = line_color,\n",
    "                     alpha=0.2) # this plots the shaded error bar\n",
    "        line_shades.append((line[0],shade))\n",
    "            \n",
    "axs.set_ylabel(ylabel, fontsize=axis_label_size)\n",
    "axs.set_xlabel('Time [s]', fontsize=axis_label_size);\n",
    "axs.legend(conditions);\n",
    "axs.legend(line_shades, conditions, fontsize=15)\n",
    "axs.axvline(0, color='0.5', alpha=0.65) # plot vertical line for time zero\n",
    "axs.annotate('', xy=(timing_dict['event_bound_ratio'][0], -0.01), xycoords='axes fraction', \n",
    "                               xytext=(timing_dict['event_bound_ratio'][1], -0.01), \n",
    "                               arrowprops=dict(arrowstyle=\"-\", color='g'))\n",
    "axs.tick_params(axis = 'both', which = 'major', labelsize = tick_font_size+3)\n",
    "axs.autoscale(enable=True, axis='both', tight=True)\n",
    "\n",
    "#axs.set_ylim([-1.5, 10])\n",
    "\n",
    "if fparams['flag_save_figs']:\n",
    "        fig.savefig(os.path.join(fparams['figs_save_dir'],'roi_trial_avg_trace.png')); fig.savefig(os.path.join(fparams['figs_save_dir'],'roi_trial_avg_trace.pdf'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c73d720-9e65-4ee0-a9db-89f8b58c0b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d498b-a503-48c4-937c-3914897cd9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ef519-e9ee-47d7-9997-dbfeee6599a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a6cefe-62b6-4edf-8ed3-063c79c848ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a test\n",
    "with open(os.path.join(save_dir, 'event_data_dict - Copy.pkl'), 'rb') as file:\n",
    "        copy_dat = pickle.load(file)\n",
    "\n",
    "with open(os.path.join(save_dir, 'event_data_dict.pkl'), 'rb') as file:\n",
    "        main_dat = pickle.load(file)\n",
    "\n",
    "print(copy_dat['minus']['data'].shape)\n",
    "\n",
    "print(main_dat['minus']['data'].shape)\n",
    "\n",
    "np.array_equal(copy_dat['minus']['data'], main_dat['minus']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03313308-10d7-41f3-ba32-e2b18cfa475b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
